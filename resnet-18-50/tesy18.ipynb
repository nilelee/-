{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "# data = unpickle('./CIFAR10/cifar-10-batches-py/test_batch')\n",
    "#\n",
    "# data[b'data'][0] # array([158, 159, 165, ..., 124, 129, 110], dtype=uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Residual(nn.Module):\n",
    "    # 残差块\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "\n",
    "# ResNet-18\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5,                     #Squential是个有序的容器，网络层将按照传入该容器的顺序依次加入，用[]来访问任意一层\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./datebase/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/26421880 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15b6b67fcdfb48fd9bea0ab6162459e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datebase/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./datebase/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./datebase/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/29515 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14305c0de30d4ed2bbc3493230a9ba8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datebase/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./datebase/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./datebase/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4422102 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d21ea3aa6c43415fb4c3bc95a9a15df9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datebase/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datebase/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./datebase/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5148 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea1bfa38d7d24158b39f920a7466e79a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datebase/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datebase/FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 下载并配置数据集\n",
    "trans = transforms.Compose(\n",
    "    [transforms.Resize((224,224)), transforms.ToTensor()])#和d2l不同 重塑为224\n",
    "train_dataset = datasets.FashionMNIST( root=r'./datebase/',\n",
    "     train=True, transform=trans, download=True)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=r'./datebase/', train=False, transform=trans, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "# batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            # nn.init.xavier_uniform_(m.weight )#80.87%\n",
    "            nn.init.kaiming_uniform_(m.weight)#\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    # net.load_state_dict(torch.load(\"./resnet18-f37072fd.pth\"),strict=False)\n",
    "\n",
    "    print(f'Training on:[{device}]')\n",
    "    net.to(device)#.to(device) 可以指定CPU 或者GPU\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()#在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout 。\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 30) == 0 or i == num_batches - 1:\n",
    "                print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {train_l:.4f}')\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(            f'Train Accuracy: {train_acc*100:.2f}%, Test Accuracy: {test_acc*100:.2f}%')\n",
    "    print(f'{metric[2] * epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on: [{str(device)}]')\n",
    "    torch.save(net.state_dict(),\n",
    "               f\".\\\\model\\\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    # torch.save(net,f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:[cuda:0]\n",
      "Epoch: 1, Step: 31, Loss: 1.6733\n",
      "Epoch: 1, Step: 62, Loss: 1.2108\n",
      "Epoch: 1, Step: 93, Loss: 1.0021\n",
      "Epoch: 1, Step: 124, Loss: 0.8934\n",
      "Epoch: 1, Step: 155, Loss: 0.8192\n",
      "Epoch: 1, Step: 186, Loss: 0.7670\n",
      "Epoch: 1, Step: 217, Loss: 0.7277\n",
      "Epoch: 1, Step: 248, Loss: 0.6888\n",
      "Epoch: 1, Step: 279, Loss: 0.6638\n",
      "Epoch: 1, Step: 310, Loss: 0.6365\n",
      "Epoch: 1, Step: 341, Loss: 0.6182\n",
      "Epoch: 1, Step: 372, Loss: 0.6006\n",
      "Epoch: 1, Step: 403, Loss: 0.5815\n",
      "Epoch: 1, Step: 434, Loss: 0.5674\n",
      "Epoch: 1, Step: 465, Loss: 0.5527\n",
      "Epoch: 1, Step: 496, Loss: 0.5393\n",
      "Epoch: 1, Step: 527, Loss: 0.5276\n",
      "Epoch: 1, Step: 558, Loss: 0.5158\n",
      "Epoch: 1, Step: 589, Loss: 0.5050\n",
      "Epoch: 1, Step: 620, Loss: 0.4947\n",
      "Epoch: 1, Step: 651, Loss: 0.4845\n",
      "Epoch: 1, Step: 682, Loss: 0.4773\n",
      "Epoch: 1, Step: 713, Loss: 0.4716\n",
      "Epoch: 1, Step: 744, Loss: 0.4654\n",
      "Epoch: 1, Step: 775, Loss: 0.4597\n",
      "Epoch: 1, Step: 806, Loss: 0.4528\n",
      "Epoch: 1, Step: 837, Loss: 0.4466\n",
      "Epoch: 1, Step: 868, Loss: 0.4414\n",
      "Epoch: 1, Step: 899, Loss: 0.4362\n",
      "Epoch: 1, Step: 930, Loss: 0.4314\n",
      "Epoch: 1, Step: 938, Loss: 0.4302\n",
      "Train Accuracy: 84.84%, Test Accuracy: 89.87%\n",
      "Epoch: 2, Step: 31, Loss: 0.2356\n",
      "Epoch: 2, Step: 62, Loss: 0.2507\n",
      "Epoch: 2, Step: 93, Loss: 0.2634\n",
      "Epoch: 2, Step: 124, Loss: 0.2618\n",
      "Epoch: 2, Step: 155, Loss: 0.2586\n",
      "Epoch: 2, Step: 186, Loss: 0.2553\n",
      "Epoch: 2, Step: 217, Loss: 0.2532\n",
      "Epoch: 2, Step: 248, Loss: 0.2562\n",
      "Epoch: 2, Step: 279, Loss: 0.2559\n",
      "Epoch: 2, Step: 310, Loss: 0.2542\n",
      "Epoch: 2, Step: 341, Loss: 0.2515\n",
      "Epoch: 2, Step: 372, Loss: 0.2541\n",
      "Epoch: 2, Step: 403, Loss: 0.2525\n",
      "Epoch: 2, Step: 434, Loss: 0.2523\n",
      "Epoch: 2, Step: 465, Loss: 0.2525\n",
      "Epoch: 2, Step: 496, Loss: 0.2512\n",
      "Epoch: 2, Step: 527, Loss: 0.2516\n",
      "Epoch: 2, Step: 558, Loss: 0.2517\n",
      "Epoch: 2, Step: 589, Loss: 0.2514\n",
      "Epoch: 2, Step: 620, Loss: 0.2514\n",
      "Epoch: 2, Step: 651, Loss: 0.2508\n",
      "Epoch: 2, Step: 682, Loss: 0.2506\n",
      "Epoch: 2, Step: 713, Loss: 0.2503\n",
      "Epoch: 2, Step: 744, Loss: 0.2485\n",
      "Epoch: 2, Step: 775, Loss: 0.2477\n",
      "Epoch: 2, Step: 806, Loss: 0.2483\n",
      "Epoch: 2, Step: 837, Loss: 0.2485\n",
      "Epoch: 2, Step: 868, Loss: 0.2474\n",
      "Epoch: 2, Step: 899, Loss: 0.2473\n",
      "Epoch: 2, Step: 930, Loss: 0.2472\n",
      "Epoch: 2, Step: 938, Loss: 0.2473\n",
      "Train Accuracy: 91.01%, Test Accuracy: 87.97%\n",
      "Epoch: 3, Step: 31, Loss: 0.2041\n",
      "Epoch: 3, Step: 62, Loss: 0.2036\n",
      "Epoch: 3, Step: 93, Loss: 0.1946\n",
      "Epoch: 3, Step: 124, Loss: 0.2002\n",
      "Epoch: 3, Step: 155, Loss: 0.1995\n",
      "Epoch: 3, Step: 186, Loss: 0.2017\n",
      "Epoch: 3, Step: 217, Loss: 0.2071\n",
      "Epoch: 3, Step: 248, Loss: 0.2069\n",
      "Epoch: 3, Step: 279, Loss: 0.2065\n",
      "Epoch: 3, Step: 310, Loss: 0.2053\n",
      "Epoch: 3, Step: 341, Loss: 0.2063\n",
      "Epoch: 3, Step: 372, Loss: 0.2063\n",
      "Epoch: 3, Step: 403, Loss: 0.2064\n",
      "Epoch: 3, Step: 434, Loss: 0.2061\n",
      "Epoch: 3, Step: 465, Loss: 0.2059\n",
      "Epoch: 3, Step: 496, Loss: 0.2055\n",
      "Epoch: 3, Step: 527, Loss: 0.2054\n",
      "Epoch: 3, Step: 558, Loss: 0.2057\n",
      "Epoch: 3, Step: 589, Loss: 0.2064\n",
      "Epoch: 3, Step: 620, Loss: 0.2057\n",
      "Epoch: 3, Step: 651, Loss: 0.2061\n",
      "Epoch: 3, Step: 682, Loss: 0.2060\n",
      "Epoch: 3, Step: 713, Loss: 0.2064\n",
      "Epoch: 3, Step: 744, Loss: 0.2062\n",
      "Epoch: 3, Step: 775, Loss: 0.2070\n",
      "Epoch: 3, Step: 806, Loss: 0.2067\n",
      "Epoch: 3, Step: 837, Loss: 0.2065\n",
      "Epoch: 3, Step: 868, Loss: 0.2063\n",
      "Epoch: 3, Step: 899, Loss: 0.2059\n",
      "Epoch: 3, Step: 930, Loss: 0.2061\n",
      "Epoch: 3, Step: 938, Loss: 0.2061\n",
      "Train Accuracy: 92.49%, Test Accuracy: 91.95%\n",
      "Epoch: 4, Step: 31, Loss: 0.1868\n",
      "Epoch: 4, Step: 62, Loss: 0.1844\n",
      "Epoch: 4, Step: 93, Loss: 0.1710\n",
      "Epoch: 4, Step: 124, Loss: 0.1746\n",
      "Epoch: 4, Step: 155, Loss: 0.1708\n",
      "Epoch: 4, Step: 186, Loss: 0.1723\n",
      "Epoch: 4, Step: 217, Loss: 0.1742\n",
      "Epoch: 4, Step: 248, Loss: 0.1720\n",
      "Epoch: 4, Step: 279, Loss: 0.1733\n",
      "Epoch: 4, Step: 310, Loss: 0.1741\n",
      "Epoch: 4, Step: 341, Loss: 0.1757\n",
      "Epoch: 4, Step: 372, Loss: 0.1753\n",
      "Epoch: 4, Step: 403, Loss: 0.1750\n",
      "Epoch: 4, Step: 434, Loss: 0.1755\n",
      "Epoch: 4, Step: 465, Loss: 0.1760\n",
      "Epoch: 4, Step: 496, Loss: 0.1757\n",
      "Epoch: 4, Step: 527, Loss: 0.1754\n",
      "Epoch: 4, Step: 558, Loss: 0.1758\n",
      "Epoch: 4, Step: 589, Loss: 0.1760\n",
      "Epoch: 4, Step: 620, Loss: 0.1761\n",
      "Epoch: 4, Step: 651, Loss: 0.1761\n",
      "Epoch: 4, Step: 682, Loss: 0.1753\n",
      "Epoch: 4, Step: 713, Loss: 0.1761\n",
      "Epoch: 4, Step: 744, Loss: 0.1765\n",
      "Epoch: 4, Step: 775, Loss: 0.1771\n",
      "Epoch: 4, Step: 806, Loss: 0.1774\n",
      "Epoch: 4, Step: 837, Loss: 0.1769\n",
      "Epoch: 4, Step: 868, Loss: 0.1774\n",
      "Epoch: 4, Step: 899, Loss: 0.1777\n",
      "Epoch: 4, Step: 930, Loss: 0.1775\n",
      "Epoch: 4, Step: 938, Loss: 0.1775\n",
      "Train Accuracy: 93.52%, Test Accuracy: 92.05%\n",
      "Epoch: 5, Step: 31, Loss: 0.1388\n",
      "Epoch: 5, Step: 62, Loss: 0.1482\n",
      "Epoch: 5, Step: 93, Loss: 0.1466\n",
      "Epoch: 5, Step: 124, Loss: 0.1494\n",
      "Epoch: 5, Step: 155, Loss: 0.1480\n",
      "Epoch: 5, Step: 186, Loss: 0.1453\n",
      "Epoch: 5, Step: 217, Loss: 0.1426\n",
      "Epoch: 5, Step: 248, Loss: 0.1421\n",
      "Epoch: 5, Step: 279, Loss: 0.1443\n",
      "Epoch: 5, Step: 310, Loss: 0.1439\n",
      "Epoch: 5, Step: 341, Loss: 0.1447\n",
      "Epoch: 5, Step: 372, Loss: 0.1460\n",
      "Epoch: 5, Step: 403, Loss: 0.1475\n",
      "Epoch: 5, Step: 434, Loss: 0.1467\n",
      "Epoch: 5, Step: 465, Loss: 0.1484\n",
      "Epoch: 5, Step: 496, Loss: 0.1489\n",
      "Epoch: 5, Step: 527, Loss: 0.1486\n",
      "Epoch: 5, Step: 558, Loss: 0.1483\n",
      "Epoch: 5, Step: 589, Loss: 0.1480\n",
      "Epoch: 5, Step: 620, Loss: 0.1489\n",
      "Epoch: 5, Step: 651, Loss: 0.1492\n",
      "Epoch: 5, Step: 682, Loss: 0.1499\n",
      "Epoch: 5, Step: 713, Loss: 0.1498\n",
      "Epoch: 5, Step: 744, Loss: 0.1506\n",
      "Epoch: 5, Step: 775, Loss: 0.1520\n",
      "Epoch: 5, Step: 806, Loss: 0.1525\n",
      "Epoch: 5, Step: 837, Loss: 0.1530\n",
      "Epoch: 5, Step: 868, Loss: 0.1535\n",
      "Epoch: 5, Step: 899, Loss: 0.1533\n",
      "Epoch: 5, Step: 930, Loss: 0.1534\n",
      "Epoch: 5, Step: 938, Loss: 0.1534\n",
      "Train Accuracy: 94.47%, Test Accuracy: 91.46%\n",
      "Epoch: 6, Step: 31, Loss: 0.1120\n",
      "Epoch: 6, Step: 62, Loss: 0.1121\n",
      "Epoch: 6, Step: 93, Loss: 0.1046\n",
      "Epoch: 6, Step: 124, Loss: 0.1035\n",
      "Epoch: 6, Step: 155, Loss: 0.1056\n",
      "Epoch: 6, Step: 186, Loss: 0.1085\n",
      "Epoch: 6, Step: 217, Loss: 0.1079\n",
      "Epoch: 6, Step: 248, Loss: 0.1087\n",
      "Epoch: 6, Step: 279, Loss: 0.1081\n",
      "Epoch: 6, Step: 310, Loss: 0.1072\n",
      "Epoch: 6, Step: 341, Loss: 0.1092\n",
      "Epoch: 6, Step: 372, Loss: 0.1113\n",
      "Epoch: 6, Step: 403, Loss: 0.1130\n",
      "Epoch: 6, Step: 434, Loss: 0.1160\n",
      "Epoch: 6, Step: 465, Loss: 0.1163\n",
      "Epoch: 6, Step: 496, Loss: 0.1161\n",
      "Epoch: 6, Step: 527, Loss: 0.1173\n",
      "Epoch: 6, Step: 558, Loss: 0.1176\n",
      "Epoch: 6, Step: 589, Loss: 0.1192\n",
      "Epoch: 6, Step: 620, Loss: 0.1200\n",
      "Epoch: 6, Step: 651, Loss: 0.1204\n",
      "Epoch: 6, Step: 682, Loss: 0.1218\n",
      "Epoch: 6, Step: 713, Loss: 0.1227\n",
      "Epoch: 6, Step: 744, Loss: 0.1231\n",
      "Epoch: 6, Step: 775, Loss: 0.1231\n",
      "Epoch: 6, Step: 806, Loss: 0.1230\n",
      "Epoch: 6, Step: 837, Loss: 0.1235\n",
      "Epoch: 6, Step: 868, Loss: 0.1238\n",
      "Epoch: 6, Step: 899, Loss: 0.1238\n",
      "Epoch: 6, Step: 930, Loss: 0.1240\n",
      "Epoch: 6, Step: 938, Loss: 0.1241\n",
      "Train Accuracy: 95.47%, Test Accuracy: 92.54%\n",
      "Epoch: 7, Step: 31, Loss: 0.1006\n",
      "Epoch: 7, Step: 62, Loss: 0.0929\n",
      "Epoch: 7, Step: 93, Loss: 0.0906\n",
      "Epoch: 7, Step: 124, Loss: 0.0875\n",
      "Epoch: 7, Step: 155, Loss: 0.0875\n",
      "Epoch: 7, Step: 186, Loss: 0.0880\n",
      "Epoch: 7, Step: 217, Loss: 0.0858\n",
      "Epoch: 7, Step: 248, Loss: 0.0849\n",
      "Epoch: 7, Step: 279, Loss: 0.0844\n",
      "Epoch: 7, Step: 310, Loss: 0.0836\n",
      "Epoch: 7, Step: 341, Loss: 0.0836\n",
      "Epoch: 7, Step: 372, Loss: 0.0847\n",
      "Epoch: 7, Step: 403, Loss: 0.0858\n",
      "Epoch: 7, Step: 434, Loss: 0.0871\n",
      "Epoch: 7, Step: 465, Loss: 0.0874\n",
      "Epoch: 7, Step: 496, Loss: 0.0885\n",
      "Epoch: 7, Step: 527, Loss: 0.0897\n",
      "Epoch: 7, Step: 558, Loss: 0.0902\n",
      "Epoch: 7, Step: 589, Loss: 0.0913\n",
      "Epoch: 7, Step: 620, Loss: 0.0923\n",
      "Epoch: 7, Step: 651, Loss: 0.0932\n",
      "Epoch: 7, Step: 682, Loss: 0.0935\n",
      "Epoch: 7, Step: 713, Loss: 0.0936\n",
      "Epoch: 7, Step: 744, Loss: 0.0940\n",
      "Epoch: 7, Step: 775, Loss: 0.0948\n",
      "Epoch: 7, Step: 806, Loss: 0.0957\n",
      "Epoch: 7, Step: 837, Loss: 0.0956\n",
      "Epoch: 7, Step: 868, Loss: 0.0960\n",
      "Epoch: 7, Step: 899, Loss: 0.0967\n",
      "Epoch: 7, Step: 930, Loss: 0.0972\n",
      "Epoch: 7, Step: 938, Loss: 0.0972\n",
      "Train Accuracy: 96.40%, Test Accuracy: 92.64%\n",
      "Epoch: 8, Step: 31, Loss: 0.0585\n",
      "Epoch: 8, Step: 62, Loss: 0.0601\n",
      "Epoch: 8, Step: 93, Loss: 0.0584\n",
      "Epoch: 8, Step: 124, Loss: 0.0581\n",
      "Epoch: 8, Step: 155, Loss: 0.0562\n",
      "Epoch: 8, Step: 186, Loss: 0.0563\n",
      "Epoch: 8, Step: 217, Loss: 0.0570\n",
      "Epoch: 8, Step: 248, Loss: 0.0566\n",
      "Epoch: 8, Step: 279, Loss: 0.0565\n",
      "Epoch: 8, Step: 310, Loss: 0.0579\n",
      "Epoch: 8, Step: 341, Loss: 0.0595\n",
      "Epoch: 8, Step: 372, Loss: 0.0604\n",
      "Epoch: 8, Step: 403, Loss: 0.0625\n",
      "Epoch: 8, Step: 434, Loss: 0.0632\n",
      "Epoch: 8, Step: 465, Loss: 0.0634\n",
      "Epoch: 8, Step: 496, Loss: 0.0649\n",
      "Epoch: 8, Step: 527, Loss: 0.0668\n",
      "Epoch: 8, Step: 558, Loss: 0.0669\n",
      "Epoch: 8, Step: 589, Loss: 0.0672\n",
      "Epoch: 8, Step: 620, Loss: 0.0677\n",
      "Epoch: 8, Step: 651, Loss: 0.0687\n",
      "Epoch: 8, Step: 682, Loss: 0.0691\n",
      "Epoch: 8, Step: 713, Loss: 0.0693\n",
      "Epoch: 8, Step: 744, Loss: 0.0700\n",
      "Epoch: 8, Step: 775, Loss: 0.0704\n",
      "Epoch: 8, Step: 806, Loss: 0.0702\n",
      "Epoch: 8, Step: 837, Loss: 0.0700\n",
      "Epoch: 8, Step: 868, Loss: 0.0707\n",
      "Epoch: 8, Step: 899, Loss: 0.0716\n",
      "Epoch: 8, Step: 930, Loss: 0.0716\n",
      "Epoch: 8, Step: 938, Loss: 0.0718\n",
      "Train Accuracy: 97.39%, Test Accuracy: 92.62%\n",
      "Epoch: 9, Step: 31, Loss: 0.0464\n",
      "Epoch: 9, Step: 62, Loss: 0.0412\n",
      "Epoch: 9, Step: 93, Loss: 0.0466\n",
      "Epoch: 9, Step: 124, Loss: 0.0482\n",
      "Epoch: 9, Step: 155, Loss: 0.0458\n",
      "Epoch: 9, Step: 186, Loss: 0.0467\n",
      "Epoch: 9, Step: 217, Loss: 0.0485\n",
      "Epoch: 9, Step: 248, Loss: 0.0480\n",
      "Epoch: 9, Step: 279, Loss: 0.0483\n",
      "Epoch: 9, Step: 310, Loss: 0.0478\n",
      "Epoch: 9, Step: 341, Loss: 0.0474\n",
      "Epoch: 9, Step: 372, Loss: 0.0489\n",
      "Epoch: 9, Step: 403, Loss: 0.0486\n",
      "Epoch: 9, Step: 434, Loss: 0.0496\n",
      "Epoch: 9, Step: 465, Loss: 0.0500\n",
      "Epoch: 9, Step: 496, Loss: 0.0495\n",
      "Epoch: 9, Step: 527, Loss: 0.0497\n",
      "Epoch: 9, Step: 558, Loss: 0.0504\n",
      "Epoch: 9, Step: 589, Loss: 0.0512\n",
      "Epoch: 9, Step: 620, Loss: 0.0515\n",
      "Epoch: 9, Step: 651, Loss: 0.0512\n",
      "Epoch: 9, Step: 682, Loss: 0.0515\n",
      "Epoch: 9, Step: 713, Loss: 0.0515\n",
      "Epoch: 9, Step: 744, Loss: 0.0518\n",
      "Epoch: 9, Step: 775, Loss: 0.0525\n",
      "Epoch: 9, Step: 806, Loss: 0.0533\n",
      "Epoch: 9, Step: 837, Loss: 0.0536\n",
      "Epoch: 9, Step: 868, Loss: 0.0537\n",
      "Epoch: 9, Step: 899, Loss: 0.0541\n",
      "Epoch: 9, Step: 930, Loss: 0.0542\n",
      "Epoch: 9, Step: 938, Loss: 0.0540\n",
      "Train Accuracy: 98.05%, Test Accuracy: 92.37%\n",
      "Epoch: 10, Step: 31, Loss: 0.0303\n",
      "Epoch: 10, Step: 62, Loss: 0.0345\n",
      "Epoch: 10, Step: 93, Loss: 0.0345\n",
      "Epoch: 10, Step: 124, Loss: 0.0329\n",
      "Epoch: 10, Step: 155, Loss: 0.0336\n",
      "Epoch: 10, Step: 186, Loss: 0.0336\n",
      "Epoch: 10, Step: 217, Loss: 0.0350\n",
      "Epoch: 10, Step: 248, Loss: 0.0338\n",
      "Epoch: 10, Step: 279, Loss: 0.0330\n",
      "Epoch: 10, Step: 310, Loss: 0.0336\n",
      "Epoch: 10, Step: 341, Loss: 0.0339\n",
      "Epoch: 10, Step: 372, Loss: 0.0339\n",
      "Epoch: 10, Step: 403, Loss: 0.0351\n",
      "Epoch: 10, Step: 434, Loss: 0.0353\n",
      "Epoch: 10, Step: 465, Loss: 0.0360\n",
      "Epoch: 10, Step: 496, Loss: 0.0375\n",
      "Epoch: 10, Step: 527, Loss: 0.0390\n",
      "Epoch: 10, Step: 558, Loss: 0.0397\n",
      "Epoch: 10, Step: 589, Loss: 0.0396\n",
      "Epoch: 10, Step: 620, Loss: 0.0397\n",
      "Epoch: 10, Step: 651, Loss: 0.0396\n",
      "Epoch: 10, Step: 682, Loss: 0.0397\n",
      "Epoch: 10, Step: 713, Loss: 0.0403\n",
      "Epoch: 10, Step: 744, Loss: 0.0411\n",
      "Epoch: 10, Step: 775, Loss: 0.0419\n",
      "Epoch: 10, Step: 806, Loss: 0.0422\n",
      "Epoch: 10, Step: 837, Loss: 0.0430\n",
      "Epoch: 10, Step: 868, Loss: 0.0432\n",
      "Epoch: 10, Step: 899, Loss: 0.0435\n",
      "Epoch: 10, Step: 930, Loss: 0.0446\n",
      "Epoch: 10, Step: 938, Loss: 0.0448\n",
      "Train Accuracy: 98.40%, Test Accuracy: 92.02%\n",
      "Epoch: 11, Step: 31, Loss: 0.0287\n",
      "Epoch: 11, Step: 62, Loss: 0.0313\n",
      "Epoch: 11, Step: 93, Loss: 0.0293\n",
      "Epoch: 11, Step: 124, Loss: 0.0283\n",
      "Epoch: 11, Step: 155, Loss: 0.0265\n",
      "Epoch: 11, Step: 186, Loss: 0.0282\n",
      "Epoch: 11, Step: 217, Loss: 0.0274\n",
      "Epoch: 11, Step: 248, Loss: 0.0268\n",
      "Epoch: 11, Step: 279, Loss: 0.0278\n",
      "Epoch: 11, Step: 310, Loss: 0.0276\n",
      "Epoch: 11, Step: 341, Loss: 0.0281\n",
      "Epoch: 11, Step: 372, Loss: 0.0280\n",
      "Epoch: 11, Step: 403, Loss: 0.0276\n",
      "Epoch: 11, Step: 434, Loss: 0.0283\n",
      "Epoch: 11, Step: 465, Loss: 0.0288\n",
      "Epoch: 11, Step: 496, Loss: 0.0287\n",
      "Epoch: 11, Step: 527, Loss: 0.0288\n",
      "Epoch: 11, Step: 558, Loss: 0.0285\n",
      "Epoch: 11, Step: 589, Loss: 0.0286\n",
      "Epoch: 11, Step: 620, Loss: 0.0291\n",
      "Epoch: 11, Step: 651, Loss: 0.0295\n",
      "Epoch: 11, Step: 682, Loss: 0.0298\n",
      "Epoch: 11, Step: 713, Loss: 0.0301\n",
      "Epoch: 11, Step: 744, Loss: 0.0307\n",
      "Epoch: 11, Step: 775, Loss: 0.0307\n",
      "Epoch: 11, Step: 806, Loss: 0.0311\n",
      "Epoch: 11, Step: 837, Loss: 0.0317\n",
      "Epoch: 11, Step: 868, Loss: 0.0320\n",
      "Epoch: 11, Step: 899, Loss: 0.0325\n",
      "Epoch: 11, Step: 930, Loss: 0.0329\n",
      "Epoch: 11, Step: 938, Loss: 0.0330\n",
      "Train Accuracy: 98.81%, Test Accuracy: 91.87%\n",
      "Epoch: 12, Step: 31, Loss: 0.0377\n",
      "Epoch: 12, Step: 62, Loss: 0.0298\n",
      "Epoch: 12, Step: 93, Loss: 0.0293\n",
      "Epoch: 12, Step: 124, Loss: 0.0264\n",
      "Epoch: 12, Step: 155, Loss: 0.0253\n",
      "Epoch: 12, Step: 186, Loss: 0.0254\n",
      "Epoch: 12, Step: 217, Loss: 0.0253\n",
      "Epoch: 12, Step: 248, Loss: 0.0259\n",
      "Epoch: 12, Step: 279, Loss: 0.0266\n",
      "Epoch: 12, Step: 310, Loss: 0.0263\n",
      "Epoch: 12, Step: 341, Loss: 0.0264\n",
      "Epoch: 12, Step: 372, Loss: 0.0265\n",
      "Epoch: 12, Step: 403, Loss: 0.0271\n",
      "Epoch: 12, Step: 434, Loss: 0.0267\n",
      "Epoch: 12, Step: 465, Loss: 0.0265\n",
      "Epoch: 12, Step: 496, Loss: 0.0268\n",
      "Epoch: 12, Step: 527, Loss: 0.0275\n",
      "Epoch: 12, Step: 558, Loss: 0.0280\n",
      "Epoch: 12, Step: 589, Loss: 0.0285\n",
      "Epoch: 12, Step: 620, Loss: 0.0285\n",
      "Epoch: 12, Step: 651, Loss: 0.0285\n",
      "Epoch: 12, Step: 682, Loss: 0.0287\n",
      "Epoch: 12, Step: 713, Loss: 0.0290\n",
      "Epoch: 12, Step: 744, Loss: 0.0293\n",
      "Epoch: 12, Step: 775, Loss: 0.0292\n",
      "Epoch: 12, Step: 806, Loss: 0.0294\n",
      "Epoch: 12, Step: 837, Loss: 0.0298\n",
      "Epoch: 12, Step: 868, Loss: 0.0302\n",
      "Epoch: 12, Step: 899, Loss: 0.0300\n",
      "Epoch: 12, Step: 930, Loss: 0.0302\n",
      "Epoch: 12, Step: 938, Loss: 0.0301\n",
      "Train Accuracy: 98.90%, Test Accuracy: 91.86%\n",
      "Epoch: 13, Step: 31, Loss: 0.0310\n",
      "Epoch: 13, Step: 62, Loss: 0.0315\n",
      "Epoch: 13, Step: 93, Loss: 0.0287\n",
      "Epoch: 13, Step: 124, Loss: 0.0268\n",
      "Epoch: 13, Step: 155, Loss: 0.0255\n",
      "Epoch: 13, Step: 186, Loss: 0.0256\n",
      "Epoch: 13, Step: 217, Loss: 0.0253\n",
      "Epoch: 13, Step: 248, Loss: 0.0256\n",
      "Epoch: 13, Step: 279, Loss: 0.0253\n",
      "Epoch: 13, Step: 310, Loss: 0.0244\n",
      "Epoch: 13, Step: 341, Loss: 0.0249\n",
      "Epoch: 13, Step: 372, Loss: 0.0245\n",
      "Epoch: 13, Step: 403, Loss: 0.0246\n",
      "Epoch: 13, Step: 434, Loss: 0.0241\n",
      "Epoch: 13, Step: 465, Loss: 0.0243\n",
      "Epoch: 13, Step: 496, Loss: 0.0241\n",
      "Epoch: 13, Step: 527, Loss: 0.0247\n",
      "Epoch: 13, Step: 558, Loss: 0.0250\n",
      "Epoch: 13, Step: 589, Loss: 0.0256\n",
      "Epoch: 13, Step: 620, Loss: 0.0263\n",
      "Epoch: 13, Step: 651, Loss: 0.0266\n",
      "Epoch: 13, Step: 682, Loss: 0.0269\n",
      "Epoch: 13, Step: 713, Loss: 0.0266\n",
      "Epoch: 13, Step: 744, Loss: 0.0268\n",
      "Epoch: 13, Step: 775, Loss: 0.0268\n",
      "Epoch: 13, Step: 806, Loss: 0.0267\n",
      "Epoch: 13, Step: 837, Loss: 0.0274\n",
      "Epoch: 13, Step: 868, Loss: 0.0278\n",
      "Epoch: 13, Step: 899, Loss: 0.0283\n",
      "Epoch: 13, Step: 930, Loss: 0.0285\n",
      "Epoch: 13, Step: 938, Loss: 0.0285\n",
      "Train Accuracy: 98.97%, Test Accuracy: 92.38%\n",
      "Epoch: 14, Step: 31, Loss: 0.0147\n",
      "Epoch: 14, Step: 62, Loss: 0.0169\n",
      "Epoch: 14, Step: 93, Loss: 0.0148\n",
      "Epoch: 14, Step: 124, Loss: 0.0129\n",
      "Epoch: 14, Step: 155, Loss: 0.0138\n",
      "Epoch: 14, Step: 186, Loss: 0.0126\n",
      "Epoch: 14, Step: 217, Loss: 0.0149\n",
      "Epoch: 14, Step: 248, Loss: 0.0153\n",
      "Epoch: 14, Step: 279, Loss: 0.0161\n",
      "Epoch: 14, Step: 310, Loss: 0.0157\n",
      "Epoch: 14, Step: 341, Loss: 0.0156\n",
      "Epoch: 14, Step: 372, Loss: 0.0157\n",
      "Epoch: 14, Step: 403, Loss: 0.0160\n",
      "Epoch: 14, Step: 434, Loss: 0.0163\n",
      "Epoch: 14, Step: 465, Loss: 0.0169\n",
      "Epoch: 14, Step: 496, Loss: 0.0170\n",
      "Epoch: 14, Step: 527, Loss: 0.0176\n",
      "Epoch: 14, Step: 558, Loss: 0.0183\n",
      "Epoch: 14, Step: 589, Loss: 0.0187\n",
      "Epoch: 14, Step: 620, Loss: 0.0193\n",
      "Epoch: 14, Step: 651, Loss: 0.0203\n",
      "Epoch: 14, Step: 682, Loss: 0.0207\n",
      "Epoch: 14, Step: 713, Loss: 0.0208\n",
      "Epoch: 14, Step: 744, Loss: 0.0210\n",
      "Epoch: 14, Step: 775, Loss: 0.0212\n",
      "Epoch: 14, Step: 806, Loss: 0.0213\n",
      "Epoch: 14, Step: 837, Loss: 0.0216\n",
      "Epoch: 14, Step: 868, Loss: 0.0220\n",
      "Epoch: 14, Step: 899, Loss: 0.0222\n",
      "Epoch: 14, Step: 930, Loss: 0.0222\n",
      "Epoch: 14, Step: 938, Loss: 0.0222\n",
      "Train Accuracy: 99.23%, Test Accuracy: 93.04%\n",
      "Epoch: 15, Step: 31, Loss: 0.0143\n",
      "Epoch: 15, Step: 62, Loss: 0.0143\n",
      "Epoch: 15, Step: 93, Loss: 0.0164\n",
      "Epoch: 15, Step: 124, Loss: 0.0173\n",
      "Epoch: 15, Step: 155, Loss: 0.0174\n",
      "Epoch: 15, Step: 186, Loss: 0.0171\n",
      "Epoch: 15, Step: 217, Loss: 0.0189\n",
      "Epoch: 15, Step: 248, Loss: 0.0196\n",
      "Epoch: 15, Step: 279, Loss: 0.0214\n",
      "Epoch: 15, Step: 310, Loss: 0.0212\n",
      "Epoch: 15, Step: 341, Loss: 0.0214\n",
      "Epoch: 15, Step: 372, Loss: 0.0208\n",
      "Epoch: 15, Step: 403, Loss: 0.0203\n",
      "Epoch: 15, Step: 434, Loss: 0.0205\n",
      "Epoch: 15, Step: 465, Loss: 0.0210\n",
      "Epoch: 15, Step: 496, Loss: 0.0214\n",
      "Epoch: 15, Step: 527, Loss: 0.0211\n",
      "Epoch: 15, Step: 558, Loss: 0.0209\n",
      "Epoch: 15, Step: 589, Loss: 0.0205\n",
      "Epoch: 15, Step: 620, Loss: 0.0202\n",
      "Epoch: 15, Step: 651, Loss: 0.0198\n",
      "Epoch: 15, Step: 682, Loss: 0.0196\n",
      "Epoch: 15, Step: 713, Loss: 0.0199\n",
      "Epoch: 15, Step: 744, Loss: 0.0203\n",
      "Epoch: 15, Step: 775, Loss: 0.0208\n",
      "Epoch: 15, Step: 806, Loss: 0.0221\n",
      "Epoch: 15, Step: 837, Loss: 0.0221\n",
      "Epoch: 15, Step: 868, Loss: 0.0222\n",
      "Epoch: 15, Step: 899, Loss: 0.0223\n",
      "Epoch: 15, Step: 930, Loss: 0.0225\n",
      "Epoch: 15, Step: 938, Loss: 0.0226\n",
      "Train Accuracy: 99.22%, Test Accuracy: 92.45%\n",
      "Epoch: 16, Step: 31, Loss: 0.0162\n",
      "Epoch: 16, Step: 62, Loss: 0.0157\n",
      "Epoch: 16, Step: 93, Loss: 0.0167\n",
      "Epoch: 16, Step: 124, Loss: 0.0176\n",
      "Epoch: 16, Step: 155, Loss: 0.0164\n",
      "Epoch: 16, Step: 186, Loss: 0.0164\n",
      "Epoch: 16, Step: 217, Loss: 0.0159\n",
      "Epoch: 16, Step: 248, Loss: 0.0149\n",
      "Epoch: 16, Step: 279, Loss: 0.0146\n",
      "Epoch: 16, Step: 310, Loss: 0.0142\n",
      "Epoch: 16, Step: 341, Loss: 0.0135\n",
      "Epoch: 16, Step: 372, Loss: 0.0134\n",
      "Epoch: 16, Step: 403, Loss: 0.0130\n",
      "Epoch: 16, Step: 434, Loss: 0.0129\n",
      "Epoch: 16, Step: 465, Loss: 0.0125\n",
      "Epoch: 16, Step: 496, Loss: 0.0124\n",
      "Epoch: 16, Step: 527, Loss: 0.0123\n",
      "Epoch: 16, Step: 558, Loss: 0.0125\n",
      "Epoch: 16, Step: 589, Loss: 0.0129\n",
      "Epoch: 16, Step: 620, Loss: 0.0135\n",
      "Epoch: 16, Step: 651, Loss: 0.0143\n",
      "Epoch: 16, Step: 682, Loss: 0.0142\n",
      "Epoch: 16, Step: 713, Loss: 0.0142\n",
      "Epoch: 16, Step: 744, Loss: 0.0145\n",
      "Epoch: 16, Step: 775, Loss: 0.0147\n",
      "Epoch: 16, Step: 806, Loss: 0.0151\n",
      "Epoch: 16, Step: 837, Loss: 0.0155\n",
      "Epoch: 16, Step: 868, Loss: 0.0163\n",
      "Epoch: 16, Step: 899, Loss: 0.0171\n",
      "Epoch: 16, Step: 930, Loss: 0.0181\n",
      "Epoch: 16, Step: 938, Loss: 0.0184\n",
      "Train Accuracy: 99.37%, Test Accuracy: 92.39%\n",
      "Epoch: 17, Step: 31, Loss: 0.0278\n",
      "Epoch: 17, Step: 62, Loss: 0.0256\n",
      "Epoch: 17, Step: 93, Loss: 0.0228\n",
      "Epoch: 17, Step: 124, Loss: 0.0206\n",
      "Epoch: 17, Step: 155, Loss: 0.0194\n",
      "Epoch: 17, Step: 186, Loss: 0.0202\n",
      "Epoch: 17, Step: 217, Loss: 0.0200\n",
      "Epoch: 17, Step: 248, Loss: 0.0204\n",
      "Epoch: 17, Step: 279, Loss: 0.0209\n",
      "Epoch: 17, Step: 310, Loss: 0.0208\n",
      "Epoch: 17, Step: 341, Loss: 0.0204\n",
      "Epoch: 17, Step: 372, Loss: 0.0209\n",
      "Epoch: 17, Step: 403, Loss: 0.0215\n",
      "Epoch: 17, Step: 434, Loss: 0.0210\n",
      "Epoch: 17, Step: 465, Loss: 0.0211\n",
      "Epoch: 17, Step: 496, Loss: 0.0209\n",
      "Epoch: 17, Step: 527, Loss: 0.0211\n",
      "Epoch: 17, Step: 558, Loss: 0.0210\n",
      "Epoch: 17, Step: 589, Loss: 0.0207\n",
      "Epoch: 17, Step: 620, Loss: 0.0205\n",
      "Epoch: 17, Step: 651, Loss: 0.0205\n",
      "Epoch: 17, Step: 682, Loss: 0.0208\n",
      "Epoch: 17, Step: 713, Loss: 0.0209\n",
      "Epoch: 17, Step: 744, Loss: 0.0211\n",
      "Epoch: 17, Step: 775, Loss: 0.0213\n",
      "Epoch: 17, Step: 806, Loss: 0.0211\n",
      "Epoch: 17, Step: 837, Loss: 0.0214\n",
      "Epoch: 17, Step: 868, Loss: 0.0212\n",
      "Epoch: 17, Step: 899, Loss: 0.0211\n",
      "Epoch: 17, Step: 930, Loss: 0.0211\n",
      "Epoch: 17, Step: 938, Loss: 0.0212\n",
      "Train Accuracy: 99.26%, Test Accuracy: 91.83%\n",
      "Epoch: 18, Step: 31, Loss: 0.0083\n",
      "Epoch: 18, Step: 62, Loss: 0.0122\n",
      "Epoch: 18, Step: 93, Loss: 0.0109\n",
      "Epoch: 18, Step: 124, Loss: 0.0109\n",
      "Epoch: 18, Step: 155, Loss: 0.0107\n",
      "Epoch: 18, Step: 186, Loss: 0.0107\n",
      "Epoch: 18, Step: 217, Loss: 0.0109\n",
      "Epoch: 18, Step: 248, Loss: 0.0118\n",
      "Epoch: 18, Step: 279, Loss: 0.0117\n",
      "Epoch: 18, Step: 310, Loss: 0.0125\n",
      "Epoch: 18, Step: 341, Loss: 0.0124\n",
      "Epoch: 18, Step: 372, Loss: 0.0126\n",
      "Epoch: 18, Step: 403, Loss: 0.0126\n",
      "Epoch: 18, Step: 434, Loss: 0.0127\n",
      "Epoch: 18, Step: 465, Loss: 0.0128\n",
      "Epoch: 18, Step: 496, Loss: 0.0133\n",
      "Epoch: 18, Step: 527, Loss: 0.0137\n",
      "Epoch: 18, Step: 558, Loss: 0.0137\n",
      "Epoch: 18, Step: 589, Loss: 0.0141\n",
      "Epoch: 18, Step: 620, Loss: 0.0139\n",
      "Epoch: 18, Step: 651, Loss: 0.0141\n",
      "Epoch: 18, Step: 682, Loss: 0.0138\n",
      "Epoch: 18, Step: 713, Loss: 0.0143\n",
      "Epoch: 18, Step: 744, Loss: 0.0144\n",
      "Epoch: 18, Step: 775, Loss: 0.0149\n",
      "Epoch: 18, Step: 806, Loss: 0.0151\n",
      "Epoch: 18, Step: 837, Loss: 0.0154\n",
      "Epoch: 18, Step: 868, Loss: 0.0155\n",
      "Epoch: 18, Step: 899, Loss: 0.0158\n",
      "Epoch: 18, Step: 930, Loss: 0.0163\n",
      "Epoch: 18, Step: 938, Loss: 0.0164\n",
      "Train Accuracy: 99.45%, Test Accuracy: 92.75%\n",
      "Epoch: 19, Step: 31, Loss: 0.0219\n",
      "Epoch: 19, Step: 62, Loss: 0.0199\n",
      "Epoch: 19, Step: 93, Loss: 0.0168\n",
      "Epoch: 19, Step: 124, Loss: 0.0153\n",
      "Epoch: 19, Step: 155, Loss: 0.0141\n",
      "Epoch: 19, Step: 186, Loss: 0.0140\n",
      "Epoch: 19, Step: 217, Loss: 0.0144\n",
      "Epoch: 19, Step: 248, Loss: 0.0149\n",
      "Epoch: 19, Step: 279, Loss: 0.0149\n",
      "Epoch: 19, Step: 310, Loss: 0.0142\n",
      "Epoch: 19, Step: 341, Loss: 0.0141\n",
      "Epoch: 19, Step: 372, Loss: 0.0138\n",
      "Epoch: 19, Step: 403, Loss: 0.0135\n",
      "Epoch: 19, Step: 434, Loss: 0.0138\n",
      "Epoch: 19, Step: 465, Loss: 0.0140\n",
      "Epoch: 19, Step: 496, Loss: 0.0144\n",
      "Epoch: 19, Step: 527, Loss: 0.0143\n",
      "Epoch: 19, Step: 558, Loss: 0.0144\n",
      "Epoch: 19, Step: 589, Loss: 0.0143\n",
      "Epoch: 19, Step: 620, Loss: 0.0146\n",
      "Epoch: 19, Step: 651, Loss: 0.0151\n",
      "Epoch: 19, Step: 682, Loss: 0.0159\n",
      "Epoch: 19, Step: 713, Loss: 0.0162\n",
      "Epoch: 19, Step: 744, Loss: 0.0166\n",
      "Epoch: 19, Step: 775, Loss: 0.0169\n",
      "Epoch: 19, Step: 806, Loss: 0.0170\n",
      "Epoch: 19, Step: 837, Loss: 0.0167\n",
      "Epoch: 19, Step: 868, Loss: 0.0167\n",
      "Epoch: 19, Step: 899, Loss: 0.0168\n",
      "Epoch: 19, Step: 930, Loss: 0.0167\n",
      "Epoch: 19, Step: 938, Loss: 0.0167\n",
      "Train Accuracy: 99.40%, Test Accuracy: 93.08%\n",
      "Epoch: 20, Step: 31, Loss: 0.0095\n",
      "Epoch: 20, Step: 62, Loss: 0.0130\n",
      "Epoch: 20, Step: 93, Loss: 0.0123\n",
      "Epoch: 20, Step: 124, Loss: 0.0110\n",
      "Epoch: 20, Step: 155, Loss: 0.0112\n",
      "Epoch: 20, Step: 186, Loss: 0.0103\n",
      "Epoch: 20, Step: 217, Loss: 0.0104\n",
      "Epoch: 20, Step: 248, Loss: 0.0107\n",
      "Epoch: 20, Step: 279, Loss: 0.0109\n",
      "Epoch: 20, Step: 310, Loss: 0.0110\n",
      "Epoch: 20, Step: 341, Loss: 0.0115\n",
      "Epoch: 20, Step: 372, Loss: 0.0124\n",
      "Epoch: 20, Step: 403, Loss: 0.0135\n",
      "Epoch: 20, Step: 434, Loss: 0.0146\n",
      "Epoch: 20, Step: 465, Loss: 0.0152\n",
      "Epoch: 20, Step: 496, Loss: 0.0151\n",
      "Epoch: 20, Step: 527, Loss: 0.0148\n",
      "Epoch: 20, Step: 558, Loss: 0.0150\n",
      "Epoch: 20, Step: 589, Loss: 0.0152\n",
      "Epoch: 20, Step: 620, Loss: 0.0159\n",
      "Epoch: 20, Step: 651, Loss: 0.0167\n",
      "Epoch: 20, Step: 682, Loss: 0.0169\n",
      "Epoch: 20, Step: 713, Loss: 0.0172\n",
      "Epoch: 20, Step: 744, Loss: 0.0175\n",
      "Epoch: 20, Step: 775, Loss: 0.0177\n",
      "Epoch: 20, Step: 806, Loss: 0.0181\n",
      "Epoch: 20, Step: 837, Loss: 0.0182\n",
      "Epoch: 20, Step: 868, Loss: 0.0182\n",
      "Epoch: 20, Step: 899, Loss: 0.0180\n",
      "Epoch: 20, Step: 930, Loss: 0.0181\n",
      "Epoch: 20, Step: 938, Loss: 0.0181\n",
      "Train Accuracy: 99.38%, Test Accuracy: 93.15%\n",
      "53.3 examples/sec on: [cuda:0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory .\\model\\ does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m epochs, lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m0.001\u001B[39m\u001B[38;5;66;03m#epochs,lr = 20, 0.001\u001B[39;00m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[21], line 38\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, train_iter, test_iter, epochs, lr, device)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28mprint\u001B[39m(            \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%, Test Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39mepochs\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mtimer\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m examples/sec \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     37\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mon: [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(device)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 38\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m           \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mResNet-18_CIFAR-10_Epoch\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepochs\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_Accuracy\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mtest_acc\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[38;5;124;43m.2f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m%.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:422\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[0;32m    419\u001B[0m _check_dill_version(pickle_module)\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m--> 422\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_zipfile_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m    423\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001B[0;32m    424\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:309\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[1;34m(name_or_buffer)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    308\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[1;32m--> 309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\serialization.py:287\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 287\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_zipfile_writer_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPyTorchFileWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Parent directory .\\model\\ does not exist."
     ]
    }
   ],
   "source": [
    "# 下载并配置数据集\n",
    "trans = transforms.Compose(\n",
    "    [transforms.Resize((224,224)), transforms.ToTensor()])#和d2l不同 重塑为224\n",
    "train_dataset = datasets.FashionMNIST( root=r'./datebase/',\n",
    "     train=True, transform=trans, download=True)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=r'./datebase/', train=False, transform=trans, download=True)\n",
    "# 配置数据加载器\n",
    "# batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=64, shuffle=True)\n",
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "train(net, train_loader, test_loader, epochs, lr, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
