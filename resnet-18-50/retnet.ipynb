{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T09:20:03.719445Z",
     "start_time": "2024-04-08T09:20:01.053225Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "# data = unpickle('./CIFAR10/cifar-10-batches-py/test_batch')\n",
    "#\n",
    "# data[b'data'][0] # array([158, 159, 165, ..., 124, 129, 110], dtype=uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ret50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34c05150544f1bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# 1x1 conv -> 3x3 conv -> 1x1 conv\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, channels, stride=1, use_1x1conv=False):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, channels*4, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channels*4)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.Conv2d(\n",
    "                in_channels, channels*4, kernel_size=1, stride=stride\n",
    "            )\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x1 conv 通道数：in_channels -> channels\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 3x3 conv 通道数：channels -> channels\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        # 1x1 conv 通道数: channels -> 4*channels\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        # 恒等映射 or 1x1 conv\n",
    "        if  self.conv4 == None:\n",
    "            identity = x\n",
    "        else:\n",
    "            identity = self.conv4(x)\n",
    "\n",
    "\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "def bottleneck_block(in_channels, channels, num_bottlenecks, not_FirstBlock = True):\n",
    "    # 第一个neck使用1x1conv，剩余的neck不使用1x1conv\n",
    "    # 第一个block的stride=1，后面的block的stride=2\n",
    "    blk = []\n",
    "    for i in range(num_bottlenecks):\n",
    "        if i == 0:\n",
    "            blk.append(\n",
    "                Bottleneck(in_channels, channels, stride=not_FirstBlock+1, use_1x1conv=True)\n",
    "            )\n",
    "        else:\n",
    "            blk.append(\n",
    "                Bottleneck(channels*4, channels)\n",
    "            )\n",
    "    return blk\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(*bottleneck_block(64, 64, 3, not_FirstBlock=False))\n",
    "b3 = nn.Sequential(*bottleneck_block(64*4, 128, 3))\n",
    "b4 = nn.Sequential(*bottleneck_block(128*4, 256, 3))\n",
    "b5 = nn.Sequential(*bottleneck_block(256*4, 512, 3))\n",
    "ret50 = nn.Sequential(\n",
    "    b1, b2, b3, b4, b5,\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:37:03.038517Z",
     "start_time": "2024-03-25T02:37:02.968175Z"
    }
   },
   "id": "6ea7a1be9a54bd09",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用28*28像素 进行训练分类"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "219a3c6a3f15e32b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trans24 = transforms.Compose(\n",
    "    [transforms.ToTensor()])#28*28\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=r'./', train=True, transform=trans24, download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=r'./', train=False, transform=trans24, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:37:49.559200Z",
     "start_time": "2024-03-25T02:37:49.522244Z"
    }
   },
   "id": "6d24a1f8b12e1e77",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练完成后会保存模型，可以修改模型的保存路径。\n",
    "\n",
    "# 保存和加载整个模型\n",
    "torch.save(net, 'net.pth')\n",
    "model = torch.load('net.pth')\n",
    "\n",
    "# 将my_resnet模型储存为my_resnet.pth\n",
    "torch.save(net.state_dict(), \"net_parameter.pth\")\n",
    "# 加载resnet，模型存放在my_resnet.pth\n",
    "net.load_state_dict(torch.load(\"net_parameter.pth\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa97d3c7d787a2b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def train(net, train_iter, test_iter, epochs,scheduler=None, device):\n",
    "\n",
    "\n",
    "# device = d2l.try_gpu()\n",
    "device = torch.device(\"cuda:0\")\n",
    "def train_loss(net, train_iter, test_iter, epochs, loss,  device, lr):\n",
    "# 参数初始化1\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            # nn.init.xavier_uniform_(m.weight )#80.87%\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    # net.load_state_dict(torch.load(\"./resnet18-f37072fd.pth\"),strict=False)\n",
    "\n",
    "\n",
    "    net.to(device)#.to(device) 可以指定CPU 或者GPU\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[0, epochs],#画出loos新添加画图工具\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()#在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout 。\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            # 画出loos新添加\n",
    "            if (i + 1) % 50 == 0:\n",
    "                animator.add(epoch + i / len(train_iter),\n",
    "                             (train_loss, train_acc, None))\n",
    "\n",
    "            if (i + 1) % (num_batches // 30) == 0 or i == num_batches - 1:\n",
    "                print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {train_loss:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        # 画出loos新添加\n",
    "        animator.add(epoch+1, (None, None, test_acc))\n",
    "\n",
    "        #新添加 学习率调度器 多因子调度器\n",
    "\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(),\n",
    "               f\".\\\\model\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net,f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:38:01.818266Z",
     "start_time": "2024-03-25T02:38:01.809541Z"
    }
   },
   "id": "d0da842ba17de64c",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb65349e1f5427e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            # nn.init.xavier_uniform_(m.weight )#80.87%d2l使用的\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    # net.load_state_dict(torch.load(\"./resnet18-f37072fd.pth\"),strict=False)\n",
    "\n",
    "    print(f'Training on:[{device}]')\n",
    "    net.to(device)#.to(device) 可以指定CPU 或者GPU\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "    # loss = []\n",
    "    # acc  = []\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()#在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout 。\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            # train_l.tolist()\n",
    "            # train_acc.tolist()\n",
    "            # loss.append(train_l)\n",
    "            # acc.append(train_acc)\n",
    "\n",
    "            if (i + 1) % (num_batches // 30) == 0 or i == num_batches - 1:\n",
    "                print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {train_l:.4f}')\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(            f'Train Accuracy: {train_acc*100:.2f}%, Test Accuracy: {test_acc*100:.2f}%')\n",
    "    print(f'{metric[2] * epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on: [{str(device)}]')\n",
    "    # torch.save(net.state_dict(),\n",
    "    #            f\".\\\\model\\\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    # torch.save(net,f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2549827b1fb5ad5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9de96ece9f51e0da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 在Pytorch中构建好一个模型后，一般需要进行预训练权重中加载。torch.load_state_dict()函数就是用于将预训练的参数权重加载到新的模型之中，操作方式如下所示：\n",
    "# sd_net = torchvision.models.resnte50(pretrained=False)\n",
    "# sd_net.load_state_dict(torch.load('*.pth'), strict=True)\n",
    "在本博文中重点关注的是 属性 strict; 当strict=True,要求预训练权重层数的键值与新构建的模型中的权重层数名称完全吻合；如果新构建的模型在层数上进行了部分微调，则上述代码就会报错：说key对应不上。\n",
    "\n",
    "此时，如果我们采用strict=False 就能够完美的解决这个问题。也即，与训练权重中与新构建网络中匹配层的键值就进行使用，没有的就默认初始化。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc828fa87e9222"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 在Pytorch中构建好一个模型后，一般需要进行预训练权重中加载。torch.load_state_dict()函数就是用于将预训练的参数权重加载到新的模型之中，操作方式如下所示：\n",
    "# sd_net = torchvision.models.resnte50(pretrained=False)\n",
    "# sd_net.load_state_dict(torch.load('*.pth'), strict=True)\n",
    "在本博文中重点关注的是 属性 strict; 当strict=True,要求预训练权重层数的键值与新构建的模型中的权重层数名称完全吻合；如果新构建的模型在层数上进行了部分微调，则上述代码就会报错：说key对应不上。\n",
    "\n",
    "此时，如果我们采用strict=False 就能够完美的解决这个问题。也即，与训练权重中与新构建网络中匹配层的键值就进行使用，没有的就默认初始化。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d84b5081f044d0ec"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 7, Loss: 5.1789\n",
      "Epoch: 1, Step: 14, Loss: 3.3592\n",
      "Epoch: 1, Step: 21, Loss: 2.5962\n",
      "Epoch: 1, Step: 28, Loss: 2.1353\n",
      "Epoch: 1, Step: 35, Loss: 1.8598\n",
      "Epoch: 1, Step: 42, Loss: 1.6738\n",
      "Epoch: 1, Step: 49, Loss: 1.5202\n"
     ]
    }
   ],
   "source": [
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\n",
    "train_loss(ret50, train_loader, test_loader, epochs, loss, device, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-25T02:38:11.636548Z"
    }
   },
   "id": "9efb5c89e5e1de2d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c1981291f20cddd6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
