{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T12:35:44.097444Z",
     "start_time": "2024-04-10T12:35:44.082446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-10T12:35:45.611048Z",
     "start_time": "2024-04-10T12:35:45.593050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 代码有问题\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, channels, stride))\n",
    "        self.in_channels = channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "def resnet50():\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    return model\n",
    "\n",
    "def resnet110():\n",
    "    model = ResNet(Bottleneck, [18, 18, 18, 18])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ret50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ret50 分类FashionMNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 142\u001B[0m\n\u001B[0;32m    140\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m    141\u001B[0m \u001B[38;5;66;03m# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\u001B[39;00m\n\u001B[1;32m--> 142\u001B[0m \u001B[43mtrain_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresnet50\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[12], line 82\u001B[0m, in \u001B[0;36mtrain_loss\u001B[1;34m(net, train_iter, test_iter, epochs, loss, device, lr)\u001B[0m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(m) \u001B[38;5;241m==\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(m) \u001B[38;5;241m==\u001B[39m nn\u001B[38;5;241m.\u001B[39mConv2d:\n\u001B[0;32m     81\u001B[0m         nn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mkaiming_uniform_(m\u001B[38;5;241m.\u001B[39mweight)\n\u001B[1;32m---> 82\u001B[0m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m(init_weights)\n\u001B[0;32m     84\u001B[0m net\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     85\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()  \u001B[38;5;66;03m# 创建 TensorBoard writer\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'function' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "# data = unpickle('./CIFAR10/cifar-10-batches-py/test_batch')\n",
    "#\n",
    "# data[b'data'][0] # array([158, 159, 165, ..., 124, 129, 110], dtype=uint8)\n",
    "\n",
    "# 定义 Ret50\n",
    "# 1x1 conv -> 3x3 conv -> 1x1 conv\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, channels, stride))\n",
    "        self.in_channels = channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "def resnet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(Bottleneck, [18, 18, 18, 18])\n",
    "\n",
    "# 定义训练\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "def train_loss(net, train_iter, test_iter, epochs, loss, device, lr):\n",
    "    # 参数初始化\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    net.to(device)\n",
    "    writer = SummaryWriter()  # 创建 TensorBoard writer\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # 将训练损失和准确率写入 TensorBoard\n",
    "            writer.add_scalar('Train/Loss', train_loss, epoch * num_batches + i)\n",
    "            writer.add_scalar('Train/Accuracy', train_acc, epoch * num_batches + i)\n",
    "\n",
    "            # 打印训练过程\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, epoch)  # 将测试准确率写入 TensorBoard\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(), f\".\\\\model\\\\ResNet-50_Dict_FashionMNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net, f\".\\\\model\\\\ResNet-50_FashionMNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    writer.close()  # 关闭 TensorBoard writer\n",
    "    \n",
    "# 加载FashionMNIST\n",
    "trans24 = transforms.Compose(\n",
    "    [transforms.ToTensor()])#28*28\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=r'./', train=True, transform=trans24, download=True)\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=r'./', train=False, transform=trans24, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "batch_size = 4096\n",
    "train_loader64 = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader64 = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 开始训练\n",
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\n",
    "train_loss(resnet50, test_loader64, test_loader64, epochs, loss, device, lr)\n",
    "\n",
    "# tensorboard 运行\n",
    "# (mytorch) PS F:\\DeepLearn\\resnet\\runs> tensorboard --logdir=Apr10_17-45-21_NikeLee\n",
    "# 其中 'Apr10_17-45-21_NikeLee'是文件夹名称"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T06:22:24.750581Z",
     "start_time": "2024-04-14T06:22:24.644581Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CIFAR-10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1, Batch: 1, Train Loss: 5.2135, Train Acc: 0.1067\n",
      "Epoch: 1, Batch: 2, Train Loss: 12.8641, Train Acc: 0.1455\n",
      "Epoch: 1, Batch: 3, Train Loss: 13.5099, Train Acc: 0.1407\n",
      "Epoch: 2, Batch: 1, Train Loss: 10.2949, Train Acc: 0.1848\n",
      "Epoch: 2, Batch: 2, Train Loss: 8.3082, Train Acc: 0.1991\n",
      "Epoch: 2, Batch: 3, Train Loss: 7.5315, Train Acc: 0.1989\n",
      "Epoch: 3, Batch: 1, Train Loss: 3.1154, Train Acc: 0.2427\n",
      "Epoch: 3, Batch: 2, Train Loss: 2.9448, Train Acc: 0.2670\n",
      "Epoch: 3, Batch: 3, Train Loss: 2.8826, Train Acc: 0.2682\n",
      "Epoch: 4, Batch: 1, Train Loss: 2.1189, Train Acc: 0.3157\n",
      "Epoch: 4, Batch: 2, Train Loss: 2.0579, Train Acc: 0.3307\n",
      "Epoch: 4, Batch: 3, Train Loss: 2.0515, Train Acc: 0.3309\n",
      "Epoch: 5, Batch: 1, Train Loss: 1.7847, Train Acc: 0.4084\n",
      "Epoch: 5, Batch: 2, Train Loss: 1.7403, Train Acc: 0.4119\n",
      "Epoch: 5, Batch: 3, Train Loss: 1.7188, Train Acc: 0.4147\n",
      "Epoch: 6, Batch: 1, Train Loss: 1.4733, Train Acc: 0.4800\n",
      "Epoch: 6, Batch: 2, Train Loss: 1.4468, Train Acc: 0.4800\n",
      "Epoch: 6, Batch: 3, Train Loss: 1.4497, Train Acc: 0.4824\n",
      "Epoch: 7, Batch: 1, Train Loss: 1.2230, Train Acc: 0.5764\n",
      "Epoch: 7, Batch: 2, Train Loss: 1.2203, Train Acc: 0.5764\n",
      "Epoch: 7, Batch: 3, Train Loss: 1.2097, Train Acc: 0.5817\n",
      "Epoch: 8, Batch: 1, Train Loss: 0.9955, Train Acc: 0.6675\n",
      "Epoch: 8, Batch: 2, Train Loss: 0.9944, Train Acc: 0.6597\n",
      "Epoch: 8, Batch: 3, Train Loss: 0.9872, Train Acc: 0.6627\n",
      "Epoch: 9, Batch: 1, Train Loss: 0.7733, Train Acc: 0.7588\n",
      "Epoch: 9, Batch: 2, Train Loss: 0.7406, Train Acc: 0.7686\n",
      "Epoch: 9, Batch: 3, Train Loss: 0.7343, Train Acc: 0.7700\n",
      "Epoch: 10, Batch: 1, Train Loss: 0.5247, Train Acc: 0.8557\n",
      "Epoch: 10, Batch: 2, Train Loss: 0.5015, Train Acc: 0.8621\n",
      "Epoch: 10, Batch: 3, Train Loss: 0.4947, Train Acc: 0.8633\n",
      "Epoch: 11, Batch: 1, Train Loss: 0.3104, Train Acc: 0.9287\n",
      "Epoch: 11, Batch: 2, Train Loss: 0.2859, Train Acc: 0.9359\n",
      "Epoch: 11, Batch: 3, Train Loss: 0.2808, Train Acc: 0.9356\n",
      "Epoch: 12, Batch: 1, Train Loss: 0.1555, Train Acc: 0.9766\n",
      "Epoch: 12, Batch: 2, Train Loss: 0.1459, Train Acc: 0.9764\n",
      "Epoch: 12, Batch: 3, Train Loss: 0.1437, Train Acc: 0.9756\n",
      "Epoch: 13, Batch: 1, Train Loss: 0.0718, Train Acc: 0.9924\n",
      "Epoch: 13, Batch: 2, Train Loss: 0.0675, Train Acc: 0.9929\n",
      "Epoch: 13, Batch: 3, Train Loss: 0.0657, Train Acc: 0.9926\n",
      "Epoch: 14, Batch: 1, Train Loss: 0.0296, Train Acc: 0.9993\n",
      "Epoch: 14, Batch: 2, Train Loss: 0.0294, Train Acc: 0.9985\n",
      "Epoch: 14, Batch: 3, Train Loss: 0.0290, Train Acc: 0.9984\n",
      "Epoch: 15, Batch: 1, Train Loss: 0.0170, Train Acc: 0.9993\n",
      "Epoch: 15, Batch: 2, Train Loss: 0.0153, Train Acc: 0.9995\n",
      "Epoch: 15, Batch: 3, Train Loss: 0.0145, Train Acc: 0.9996\n",
      "Epoch: 16, Batch: 1, Train Loss: 0.0093, Train Acc: 0.9995\n",
      "Epoch: 16, Batch: 2, Train Loss: 0.0083, Train Acc: 0.9995\n",
      "Epoch: 16, Batch: 3, Train Loss: 0.0080, Train Acc: 0.9996\n",
      "Epoch: 17, Batch: 1, Train Loss: 0.0053, Train Acc: 0.9998\n",
      "Epoch: 17, Batch: 2, Train Loss: 0.0053, Train Acc: 0.9994\n",
      "Epoch: 17, Batch: 3, Train Loss: 0.0057, Train Acc: 0.9994\n",
      "Epoch: 18, Batch: 1, Train Loss: 0.0026, Train Acc: 1.0000\n",
      "Epoch: 18, Batch: 2, Train Loss: 0.0025, Train Acc: 1.0000\n",
      "Epoch: 18, Batch: 3, Train Loss: 0.0029, Train Acc: 0.9999\n",
      "Epoch: 19, Batch: 1, Train Loss: 0.0022, Train Acc: 1.0000\n",
      "Epoch: 19, Batch: 2, Train Loss: 0.0021, Train Acc: 1.0000\n",
      "Epoch: 19, Batch: 3, Train Loss: 0.0021, Train Acc: 1.0000\n",
      "Epoch: 20, Batch: 1, Train Loss: 0.0017, Train Acc: 1.0000\n",
      "Epoch: 20, Batch: 2, Train Loss: 0.0015, Train Acc: 1.0000\n",
      "Epoch: 20, Batch: 3, Train Loss: 0.0017, Train Acc: 0.9999\n",
      "train loss 0.002, train acc 1.000, test acc 1.000\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "# 导入必要的库\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "# data = unpickle('./CIFAR10/cifar-10-batches-py/test_batch')\n",
    "#\n",
    "# data[b'data'][0] # array([158, 159, 165, ..., 124, 129, 110], dtype=uint8)\n",
    "\n",
    "# 定义 Ret50\n",
    "# 1x1 conv -> 3x3 conv -> 1x1 conv\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, channels, stride=1, use_1x1conv=False):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, channels*4, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channels*4)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.Conv2d(\n",
    "                in_channels, channels*4, kernel_size=1, stride=stride\n",
    "            )\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x1 conv 通道数：in_channels -> channels\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 3x3 conv 通道数：channels -> channels\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        # 1x1 conv 通道数: channels -> 4*channels\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        # 恒等映射 or 1x1 conv\n",
    "        if  self.conv4 == None:\n",
    "            identity = x\n",
    "        else:\n",
    "            identity = self.conv4(x)\n",
    "\n",
    "\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "def bottleneck_block(in_channels, channels, num_bottlenecks, not_FirstBlock = True):\n",
    "    # 第一个neck使用1x1conv，剩余的neck不使用1x1conv\n",
    "    # 第一个block的stride=1，后面的block的stride=2\n",
    "    blk = []\n",
    "    for i in range(num_bottlenecks):\n",
    "        if i == 0:\n",
    "            blk.append(\n",
    "                Bottleneck(in_channels, channels, stride=not_FirstBlock+1, use_1x1conv=True)\n",
    "            )\n",
    "        else:\n",
    "            blk.append(\n",
    "                Bottleneck(channels*4, channels)\n",
    "            )\n",
    "    return blk\n",
    "b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),# 第一个参数是图片通道数 3是有色图片\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(*bottleneck_block(64, 64, 3, not_FirstBlock=False))\n",
    "b3 = nn.Sequential(*bottleneck_block(64*4, 128, 3))\n",
    "b4 = nn.Sequential(*bottleneck_block(128*4, 256, 3))\n",
    "b5 = nn.Sequential(*bottleneck_block(256*4, 512, 3))\n",
    "resnet50 = nn.Sequential(\n",
    "    b1, b2, b3, b4, b5,\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 10)\n",
    ")\n",
    "\n",
    "# 定义训练\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "def train_loss(net, train_iter, test_iter, epochs, loss, device, lr):\n",
    "    # 参数初始化kaiming_uniform_\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            # nn.init.xavier_uniform(m.weight)\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "\n",
    "    net.to(device)\n",
    "    writer = SummaryWriter()  # 创建 TensorBoard writer\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # trainer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # 将训练损失和准确率写入 TensorBoard\n",
    "            writer.add_scalar('Train/Loss', train_loss, epoch * num_batches + i)\n",
    "            writer.add_scalar('Train/Accuracy', train_acc, epoch * num_batches + i)\n",
    "\n",
    "            # 打印训练过程\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, epoch)  # 将测试准确率写入 TensorBoard\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(), f\".\\\\model\\\\ResNet-50_Dict_FashionMNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net, f\".\\\\model\\\\ResNet-50_FashionMNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    writer.close()  # 关闭 TensorBoard writer\n",
    "    \n",
    "# 加载FashionMNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 加载CIFAR-10训练数据集\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 加载CIFAR-10测试数据集\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "# batch_size = 4096:train loss 0.011, train acc 0.999, test acc 0.992 Adma优化器 lr =0.001\n",
    "# batch_size = 4096 train loss 2.303, train acc 0.100, test acc 0.100 Adma优化器 lr =0.1\n",
    "# batch_size = 4096 train loss 1.286, train acc 0.556, test acc 0.503 sgd优化器 lr =0.1\n",
    "# batch_size = 4096 train loss 0.315, train acc 0.960, test acc 0.860 sgd优化器 lr =0.001\n",
    "\n",
    "            # nn.init.xavier_uniform(m.weight)\n",
    "#  batch_size = 4096 ,train loss 1.131, train acc 0.669, test acc 0.607  sgd优化器 lr =0.001  nn.init.xavier_uniform(m.weight)\n",
    "# 配置数据加载器\n",
    "batch_size = 4096 #\n",
    "# batch_size = 128 # \n",
    "train_loader64 = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader64 = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 开始训练\n",
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\n",
    "\n",
    "train_loss(resnet50, test_loader64, test_loader64, epochs, loss, device, lr)\n",
    "\n",
    "# tensorboard 运行\n",
    "# (mytorch) PS F:\\DeepLearn\\resnet\\runs> tensorboard --logdir=Apr10_17-45-21_NikeLee\n",
    "# 其中 'Apr10_17-45-21_NikeLee'是文件夹名称"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T06:54:24.901890Z",
     "start_time": "2024-04-14T06:53:17.953963Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ret50 分类 MNIST数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "apex.optimizers.FusedLAMB requires cuda extensions",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 155\u001B[0m\n\u001B[0;32m    153\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m    154\u001B[0m \u001B[38;5;66;03m# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\u001B[39;00m\n\u001B[1;32m--> 155\u001B[0m \u001B[43mtrain_loss_LAMB\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret50\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 15\u001B[0m, in \u001B[0;36mtrain_loss_LAMB\u001B[1;34m(net, train_iter, test_iter, epochs, loss, device, lr)\u001B[0m\n\u001B[0;32m     12\u001B[0m net\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     13\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter()  \u001B[38;5;66;03m# 创建 TensorBoard writer\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mFusedLAMB\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 使用 LAMB 优化器\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# trainer = torch.optim.Adam(net.parameters(), lr=lr)\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# 训练损失之和，训练准确率之和，样本数\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\apex-0.1-py3.8.egg\\apex\\optimizers\\fused_lamb.py:82\u001B[0m, in \u001B[0;36mFusedLAMB.__init__\u001B[1;34m(self, params, lr, bias_correction, betas, eps, weight_decay, amsgrad, adam_w_mode, grad_averaging, set_grad_none, max_grad_norm, use_nvlamb)\u001B[0m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmulti_tensor_lamb \u001B[38;5;241m=\u001B[39m amp_C\u001B[38;5;241m.\u001B[39mmulti_tensor_lamb\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 82\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapex.optimizers.FusedLAMB requires cuda extensions\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madam_w_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m adam_w_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_grad_none \u001B[38;5;241m=\u001B[39m set_grad_none\n",
      "\u001B[1;31mRuntimeError\u001B[0m: apex.optimizers.FusedLAMB requires cuda extensions"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "# data = unpickle('./CIFAR10/cifar-10-batches-py/test_batch')\n",
    "#\n",
    "# data[b'data'][0] # array([158, 159, 165, ..., 124, 129, 110], dtype=uint8)\n",
    "\n",
    "# 定义 Ret50\n",
    "# 1x1 conv -> 3x3 conv -> 1x1 conv\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, channels, stride=1, use_1x1conv=False):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, channels*4, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channels*4)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.Conv2d(\n",
    "                in_channels, channels*4, kernel_size=1, stride=stride\n",
    "            )\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x1 conv 通道数：in_channels -> channels\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 3x3 conv 通道数：channels -> channels\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        # 1x1 conv 通道数: channels -> 4*channels\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        # 恒等映射 or 1x1 conv\n",
    "        if  self.conv4 == None:\n",
    "            identity = x\n",
    "        else:\n",
    "            identity = self.conv4(x)\n",
    "\n",
    "\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "def bottleneck_block(in_channels, channels, num_bottlenecks, not_FirstBlock = True):\n",
    "    # 第一个neck使用1x1conv，剩余的neck不使用1x1conv\n",
    "    # 第一个block的stride=1，后面的block的stride=2\n",
    "    blk = []\n",
    "    for i in range(num_bottlenecks):\n",
    "        if i == 0:\n",
    "            blk.append(\n",
    "                Bottleneck(in_channels, channels, stride=not_FirstBlock+1, use_1x1conv=True)\n",
    "            )\n",
    "        else:\n",
    "            blk.append(\n",
    "                Bottleneck(channels*4, channels)\n",
    "            )\n",
    "    return blk\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(*bottleneck_block(64, 64, 3, not_FirstBlock=False))\n",
    "b3 = nn.Sequential(*bottleneck_block(64*4, 128, 3))\n",
    "b4 = nn.Sequential(*bottleneck_block(128*4, 256, 3))\n",
    "b5 = nn.Sequential(*bottleneck_block(256*4, 512, 3))\n",
    "ret50 = nn.Sequential(\n",
    "    b1, b2, b3, b4, b5,\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 10)\n",
    ")\n",
    "\n",
    "# 定义训练\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "def train_loss(net, train_iter, test_iter, epochs, loss, device, lr):\n",
    "    # 参数初始化\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    net.to(device)\n",
    "    writer = SummaryWriter()  # 创建 TensorBoard writer\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # 将训练损失和准确率写入 TensorBoard\n",
    "            writer.add_scalar('Train/Loss', train_loss, epoch * num_batches + i)\n",
    "            writer.add_scalar('Train/Accuracy', train_acc, epoch * num_batches + i)\n",
    "\n",
    "            # 打印训练过程\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, epoch)  # 将测试准确率写入 TensorBoard\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(), f\".\\\\model\\\\ResNet-50_Dict_MNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net, f\".\\\\model\\\\ResNet-50_MNIST-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    writer.close()  # 关闭 TensorBoard writer\n",
    "    \n",
    "# 加载FashionMNIST\n",
    "trans24 = transforms.Compose(\n",
    "    [transforms.ToTensor()])#28*28\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=r'./', train=True, transform=trans24, download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=r'./', train=False, transform=trans24, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "batch_size = 4096\n",
    "train_loader64 = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader64 = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 开始训练\n",
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\n",
    "train_loss_LAMB(ret50, test_loader64, test_loader64, epochs, loss, device, lr)\n",
    "\n",
    "# tensorboard 运行\n",
    "# (mytorch) PS F:\\DeepLearn\\resnet\\runs> tensorboard --logdir=Apr10_17-45-21_NikeLee\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T08:39:46.696574Z",
     "start_time": "2024-04-12T08:39:44.555310Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可视化展示\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "测试集上的准确率为:11.780%\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from d2l import torch as d2l\n",
    "from torchvision import  transforms\n",
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = torch.load(\"F:\\\\DeepLearn\\\\resnet\\model\\\\ResNet-50_FashionMNIST-10_Epoch20_Accuracy100.00%.pth\")\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "costs = []\n",
    "\n",
    "# 测试网络\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader64:\n",
    "            inputs, labels = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "    print(f'测试集上的准确率为:{correct / total * 100:.3f}%')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    test()\n",
    "    # 绘制图片:此时的costs里面的数据是tensor类型,如果是gpu上跑的就是cuda,需要将他转换成array\n",
    "    print(costs)\n",
    "if torch.cuda.is_available():\n",
    "    costs = [cost.cpu().detach().numpy() for cost in costs]\n",
    "    print(costs)\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('number of iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Inception Net')\n",
    "    plt.show()\n",
    "else:\n",
    "    costs = [cost.numpy() for cost in costs]\n",
    "    print(costs)\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('number of iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Inception Net')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-11T09:22:55.310316Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 清理内存\n",
    "import torch\n",
    " \n",
    "\n",
    " \n",
    "# ... 模型训练代码 ...\n",
    " \n",
    "# 清理模型和优化器占用的内存\n",
    "del ret50\n",
    "\n",
    " \n",
    "# 清理PyTorch缓存\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T13:45:11.789131Z",
     "start_time": "2024-04-10T13:45:11.775410Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 重启Jupyter内核\n",
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义 ResNet-18 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T11:26:16.663040Z",
     "start_time": "2024-03-26T11:26:16.618880Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    # 残差块\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "\n",
    "# ResNet-18\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5,                     #Squential是个有序的容器，网络层将按照传入该容器的顺序依次加入，用[]来访问任意一层\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-10T13:30:51.156025Z",
     "start_time": "2024-04-10T13:30:51.111845Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 下载并配置数据集\n",
    "trans64 = transforms.Compose(\n",
    "    [transforms.Resize((224,224)), transforms.ToTensor()])#和d2l不同 重塑为224\n",
    "train_dataset64 = datasets.MNIST(\n",
    "    root=r'./', train=True, transform=trans64, download=True)\n",
    "test_dataset64 = datasets.MNIST(\n",
    "    root=r'./', train=False, transform=trans64, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "batch_size = 4096\n",
    "train_loader64 = DataLoader(dataset=train_dataset64,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader64 = DataLoader(dataset=test_dataset64,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用28*28像素 进行训练分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T09:34:47.363Z",
     "start_time": "2024-04-10T09:34:47.320001Z"
    }
   },
   "outputs": [],
   "source": [
    "trans24 = transforms.Compose(\n",
    "    [transforms.ToTensor()])#28*28\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=r'./', train=True, transform=trans24, download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=r'./', train=False, transform=trans24, download=True)\n",
    "\n",
    "# 配置数据加载器\n",
    "batch_size = 4096\n",
    "train_loader4096 = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader4096 = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练完成后会保存模型，可以修改模型的保存路径。\n",
    "\n",
    "# 保存和加载整个模型\n",
    "torch.save(net, 'net.pth')\n",
    "model = torch.load('net.pth')\n",
    "\n",
    "# 将my_resnet模型储存为my_resnet.pth\n",
    "torch.save(net.state_dict(), \"net_parameter.pth\")\n",
    "# 加载resnet，模型存放在my_resnet.pth\n",
    "net.load_state_dict(torch.load(\"net_parameter.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T11:26:38.976137Z",
     "start_time": "2024-03-26T11:26:38.971137Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "def train_loss(net, train_iter, test_iter, epochs, loss, device, lr):\n",
    "    # 参数初始化\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    net.to(device)\n",
    "    writer = SummaryWriter()  # 创建 TensorBoard writer\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # 将训练损失和准确率写入 TensorBoard\n",
    "            writer.add_scalar('Train/Loss', train_loss, epoch * num_batches + i)\n",
    "            writer.add_scalar('Train/Accuracy', train_acc, epoch * num_batches + i)\n",
    "\n",
    "            # 打印训练过程\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, epoch)  # 将测试准确率写入 TensorBoard\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(), f\".\\\\model\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net, f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    writer.close()  # 关闭 TensorBoard writer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T13:31:00.837039Z",
     "start_time": "2024-04-10T13:31:00.693715Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用LAMB 优化器:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorboard\n",
    "from apex.optimizers import FusedLAMB\n",
    "\n",
    "def train_loss_LAMB(net, train_iter, test_iter, epochs, loss, device, lr):\n",
    "    # 参数初始化\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    net.to(device)\n",
    "    writer = SummaryWriter()  # 创建 TensorBoard writer\n",
    "\n",
    "    trainer = FusedLAMB(net.parameters(), lr=lr)  # 使用 LAMB 优化器\n",
    "    # trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "\n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # 将训练损失和准确率写入 TensorBoard\n",
    "            writer.add_scalar('Train/Loss', train_loss, epoch * num_batches + i)\n",
    "            writer.add_scalar('Train/Accuracy', train_acc, epoch * num_batches + i)\n",
    "\n",
    "            # 打印训练过程\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, epoch)  # 将测试准确率写入 TensorBoard\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "    torch.save(net.state_dict(), f\".\\\\model\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    torch.save(net, f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "    writer.close()  # 关闭 TensorBoard writer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T08:39:09.520171Z",
     "start_time": "2024-04-12T08:39:09.499394Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T09:21:19.864672Z",
     "start_time": "2024-04-08T09:21:19.848674Z"
    }
   },
   "outputs": [],
   "source": [
    "# # def train(net, train_iter, test_iter, epochs,scheduler=None, device):\n",
    "# lr = 0.0001\n",
    "# \n",
    "# # device = d2l.try_gpu()\n",
    "# \n",
    "# def train_loss(net, train_iter, test_iter, epochs, loss,  device, lr):\n",
    "# # 参数初始化1\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#             # nn.init.xavier_uniform_(m.weight )#80.87%\n",
    "#             nn.init.kaiming_uniform_(m.weight)\n",
    "#     net.apply(init_weights)\n",
    "# \n",
    "#     # net.load_state_dict(torch.load(\"./resnet18-f37072fd.pth\"),strict=False)\n",
    "# \n",
    "# \n",
    "#     net.to(device)#.to(device) 可以指定CPU 或者GPU\n",
    "#     animator = d2l.Animator(xlabel='epoch', xlim=[0, epochs],#画出loos新添加画图工具\n",
    "#                             legend=['train loss', 'train acc', 'test acc'])\n",
    "# \n",
    "#     trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# \n",
    "# \n",
    "#     for epoch in range(epochs):\n",
    "#         # 训练损失之和，训练准确率之和，样本数\n",
    "#         metric = d2l.Accumulator(3)\n",
    "#         net.train()#在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout 。\n",
    "#         timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "#         for i, (X, y) in enumerate(train_iter):\n",
    "# \n",
    "#             trainer.zero_grad()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             y_hat = net(X)\n",
    "#             l = loss(y_hat, y)\n",
    "#             l.backward()\n",
    "#             trainer.step()\n",
    "#             with torch.no_grad():\n",
    "#                 metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "# \n",
    "#             train_loss = metric[0] / metric[2]\n",
    "#             train_acc = metric[1] / metric[2]\n",
    "#             # 画出loos新添加\n",
    "#             if (i + 1) % 50 == 0:\n",
    "#                 animator.add(epoch + i / len(train_iter),\n",
    "#                              (train_loss, train_acc, None))\n",
    "# \n",
    "#             if (i + 1) % (num_batches // 30) == 0 or i == num_batches - 1:\n",
    "#                 print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {train_loss:.4f}')\n",
    "# \n",
    "#         test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "#         animator.add(epoch+1, (None, None, test_acc))#画出loos新添加\n",
    "# \n",
    "#         #新添加 学习率调度器 多因子调度器\n",
    "# \n",
    "# \n",
    "#     print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "#           f'test acc {test_acc:.3f}')\n",
    "#     torch.save(net.state_dict(),\n",
    "#                f\".\\\\model\\\\ResNet-18_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n",
    "#     torch.save(net,f\".\\\\model\\\\net_CIFAR-10_Epoch{epochs}_Accuracy{test_acc*100:.2f}%.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T11:26:46.483087Z",
     "start_time": "2024-03-26T11:26:46.473892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(net, train_iter, test_iter, epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            # nn.init.xavier_uniform_(m.weight )#80.87%d2l使用的\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    print(f'Training on:[{device}]')\n",
    "    net.to(device)#.to(device) 可以指定CPU 或者GPU\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()#在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout 。\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "          \n",
    "           \n",
    "\n",
    "            if (i + 1) % (num_batches // 30) == 0 or i == num_batches - 1:\n",
    "                print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {train_l:.4f}')\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(            f'Train Accuracy: {train_acc*100:.2f}%, Test Accuracy: {test_acc*100:.2f}%')\n",
    "    print(f'{metric[2] * epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on: [{str(device)}]')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "训练模型（或加载模型）\n",
    "如果环境正确配置了 CUDA，则会由 GPU 进行训练。\n",
    "加载模型需要根据自身情况修改路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 在Pytorch中构建好一个模型后，一般需要进行预训练权重中加载。torch.load_state_dict()函数就是用于将预训练的参数权重加载到新的模型之中，操作方式如下所示：\n",
    "# sd_net = torchvision.models.resnte50(pretrained=False)\n",
    "# sd_net.load_state_dict(torch.load('*.pth'), strict=True)\n",
    "在本博文中重点关注的是 属性 strict; 当strict=True,要求预训练权重层数的键值与新构建的模型中的权重层数名称完全吻合；如果新构建的模型在层数上进行了部分微调，则上述代码就会报错：说key对应不上。\n",
    "\n",
    "此时，如果我们采用strict=False 就能够完美的解决这个问题。也即，与训练权重中与新构建网络中匹配层的键值就进行使用，没有的就默认初始化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_loader64是MNISTtrain_loader4096 是FashionMnist数据集\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ret18  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T09:30:51.503884Z",
     "start_time": "2024-03-21T09:23:47.669279Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:[cuda:0]\n",
      "Epoch: 1, Step: 31, Loss: 1.0478\n",
      "Epoch: 1, Step: 62, Loss: 0.6605\n",
      "Epoch: 1, Step: 93, Loss: 0.5033\n",
      "Epoch: 1, Step: 124, Loss: 0.4132\n",
      "Epoch: 1, Step: 155, Loss: 0.3553\n",
      "Epoch: 1, Step: 186, Loss: 0.3153\n",
      "Epoch: 1, Step: 217, Loss: 0.2841\n",
      "Epoch: 1, Step: 248, Loss: 0.2612\n",
      "Epoch: 1, Step: 279, Loss: 0.2413\n",
      "Epoch: 1, Step: 310, Loss: 0.2263\n",
      "Epoch: 1, Step: 341, Loss: 0.2143\n",
      "Epoch: 1, Step: 372, Loss: 0.2038\n",
      "Epoch: 1, Step: 403, Loss: 0.1931\n",
      "Epoch: 1, Step: 434, Loss: 0.1848\n",
      "Epoch: 1, Step: 465, Loss: 0.1769\n",
      "Epoch: 1, Step: 496, Loss: 0.1715\n",
      "Epoch: 1, Step: 527, Loss: 0.1670\n",
      "Epoch: 1, Step: 558, Loss: 0.1614\n",
      "Epoch: 1, Step: 589, Loss: 0.1565\n",
      "Epoch: 1, Step: 620, Loss: 0.1510\n",
      "Epoch: 1, Step: 651, Loss: 0.1451\n",
      "Epoch: 1, Step: 682, Loss: 0.1414\n",
      "Epoch: 1, Step: 713, Loss: 0.1381\n",
      "Epoch: 1, Step: 744, Loss: 0.1348\n",
      "Epoch: 1, Step: 775, Loss: 0.1312\n",
      "Epoch: 1, Step: 806, Loss: 0.1283\n",
      "Epoch: 1, Step: 837, Loss: 0.1254\n",
      "Epoch: 1, Step: 868, Loss: 0.1231\n",
      "Epoch: 1, Step: 899, Loss: 0.1208\n",
      "Epoch: 1, Step: 930, Loss: 0.1187\n",
      "Epoch: 1, Step: 938, Loss: 0.1181\n",
      "Train Accuracy: 96.42%, Test Accuracy: 98.20%\n",
      "Epoch: 2, Step: 31, Loss: 0.0533\n",
      "Epoch: 2, Step: 62, Loss: 0.0428\n",
      "Epoch: 2, Step: 93, Loss: 0.0415\n",
      "Epoch: 2, Step: 124, Loss: 0.0407\n",
      "Epoch: 2, Step: 155, Loss: 0.0392\n",
      "Epoch: 2, Step: 186, Loss: 0.0379\n",
      "Epoch: 2, Step: 217, Loss: 0.0387\n",
      "Epoch: 2, Step: 248, Loss: 0.0390\n",
      "Epoch: 2, Step: 279, Loss: 0.0421\n",
      "Epoch: 2, Step: 310, Loss: 0.0410\n",
      "Epoch: 2, Step: 341, Loss: 0.0406\n",
      "Epoch: 2, Step: 372, Loss: 0.0409\n",
      "Epoch: 2, Step: 403, Loss: 0.0407\n",
      "Epoch: 2, Step: 434, Loss: 0.0407\n",
      "Epoch: 2, Step: 465, Loss: 0.0409\n",
      "Epoch: 2, Step: 496, Loss: 0.0411\n",
      "Epoch: 2, Step: 527, Loss: 0.0412\n",
      "Epoch: 2, Step: 558, Loss: 0.0402\n",
      "Epoch: 2, Step: 589, Loss: 0.0411\n",
      "Epoch: 2, Step: 620, Loss: 0.0414\n",
      "Epoch: 2, Step: 651, Loss: 0.0409\n",
      "Epoch: 2, Step: 682, Loss: 0.0410\n",
      "Epoch: 2, Step: 713, Loss: 0.0407\n",
      "Epoch: 2, Step: 744, Loss: 0.0403\n",
      "Epoch: 2, Step: 775, Loss: 0.0401\n",
      "Epoch: 2, Step: 806, Loss: 0.0408\n",
      "Epoch: 2, Step: 837, Loss: 0.0404\n",
      "Epoch: 2, Step: 868, Loss: 0.0403\n",
      "Epoch: 2, Step: 899, Loss: 0.0404\n",
      "Epoch: 2, Step: 930, Loss: 0.0401\n",
      "Epoch: 2, Step: 938, Loss: 0.0400\n",
      "Train Accuracy: 98.73%, Test Accuracy: 99.12%\n",
      "Epoch: 3, Step: 31, Loss: 0.0293\n",
      "Epoch: 3, Step: 62, Loss: 0.0268\n",
      "Epoch: 3, Step: 93, Loss: 0.0348\n",
      "Epoch: 3, Step: 124, Loss: 0.0324\n",
      "Epoch: 3, Step: 155, Loss: 0.0297\n",
      "Epoch: 3, Step: 186, Loss: 0.0287\n",
      "Epoch: 3, Step: 217, Loss: 0.0281\n",
      "Epoch: 3, Step: 248, Loss: 0.0292\n",
      "Epoch: 3, Step: 279, Loss: 0.0305\n",
      "Epoch: 3, Step: 310, Loss: 0.0313\n",
      "Epoch: 3, Step: 341, Loss: 0.0311\n",
      "Epoch: 3, Step: 372, Loss: 0.0312\n",
      "Epoch: 3, Step: 403, Loss: 0.0310\n",
      "Epoch: 3, Step: 434, Loss: 0.0314\n",
      "Epoch: 3, Step: 465, Loss: 0.0316\n",
      "Epoch: 3, Step: 496, Loss: 0.0325\n",
      "Epoch: 3, Step: 527, Loss: 0.0321\n",
      "Epoch: 3, Step: 558, Loss: 0.0322\n",
      "Epoch: 3, Step: 589, Loss: 0.0314\n",
      "Epoch: 3, Step: 620, Loss: 0.0314\n",
      "Epoch: 3, Step: 651, Loss: 0.0317\n",
      "Epoch: 3, Step: 682, Loss: 0.0315\n",
      "Epoch: 3, Step: 713, Loss: 0.0316\n",
      "Epoch: 3, Step: 744, Loss: 0.0321\n",
      "Epoch: 3, Step: 775, Loss: 0.0319\n",
      "Epoch: 3, Step: 806, Loss: 0.0319\n",
      "Epoch: 3, Step: 837, Loss: 0.0317\n",
      "Epoch: 3, Step: 868, Loss: 0.0320\n",
      "Epoch: 3, Step: 899, Loss: 0.0316\n",
      "Epoch: 3, Step: 930, Loss: 0.0319\n",
      "Epoch: 3, Step: 938, Loss: 0.0318\n",
      "Train Accuracy: 99.02%, Test Accuracy: 98.11%\n",
      "Epoch: 4, Step: 31, Loss: 0.0262\n",
      "Epoch: 4, Step: 62, Loss: 0.0223\n",
      "Epoch: 4, Step: 93, Loss: 0.0208\n",
      "Epoch: 4, Step: 124, Loss: 0.0212\n",
      "Epoch: 4, Step: 155, Loss: 0.0210\n",
      "Epoch: 4, Step: 186, Loss: 0.0209\n",
      "Epoch: 4, Step: 217, Loss: 0.0202\n",
      "Epoch: 4, Step: 248, Loss: 0.0209\n",
      "Epoch: 4, Step: 279, Loss: 0.0220\n",
      "Epoch: 4, Step: 310, Loss: 0.0227\n",
      "Epoch: 4, Step: 341, Loss: 0.0229\n",
      "Epoch: 4, Step: 372, Loss: 0.0224\n",
      "Epoch: 4, Step: 403, Loss: 0.0227\n",
      "Epoch: 4, Step: 434, Loss: 0.0223\n",
      "Epoch: 4, Step: 465, Loss: 0.0216\n",
      "Epoch: 4, Step: 496, Loss: 0.0219\n",
      "Epoch: 4, Step: 527, Loss: 0.0219\n",
      "Epoch: 4, Step: 558, Loss: 0.0226\n",
      "Epoch: 4, Step: 589, Loss: 0.0233\n",
      "Epoch: 4, Step: 620, Loss: 0.0236\n",
      "Epoch: 4, Step: 651, Loss: 0.0238\n",
      "Epoch: 4, Step: 682, Loss: 0.0243\n",
      "Epoch: 4, Step: 713, Loss: 0.0244\n",
      "Epoch: 4, Step: 744, Loss: 0.0242\n",
      "Epoch: 4, Step: 775, Loss: 0.0241\n",
      "Epoch: 4, Step: 806, Loss: 0.0241\n",
      "Epoch: 4, Step: 837, Loss: 0.0244\n",
      "Epoch: 4, Step: 868, Loss: 0.0244\n",
      "Epoch: 4, Step: 899, Loss: 0.0245\n",
      "Epoch: 4, Step: 930, Loss: 0.0247\n",
      "Epoch: 4, Step: 938, Loss: 0.0249\n",
      "Train Accuracy: 99.25%, Test Accuracy: 99.11%\n",
      "Epoch: 5, Step: 31, Loss: 0.0283\n",
      "Epoch: 5, Step: 62, Loss: 0.0248\n",
      "Epoch: 5, Step: 93, Loss: 0.0251\n",
      "Epoch: 5, Step: 124, Loss: 0.0260\n",
      "Epoch: 5, Step: 155, Loss: 0.0254\n",
      "Epoch: 5, Step: 186, Loss: 0.0238\n",
      "Epoch: 5, Step: 217, Loss: 0.0246\n",
      "Epoch: 5, Step: 248, Loss: 0.0233\n",
      "Epoch: 5, Step: 279, Loss: 0.0237\n",
      "Epoch: 5, Step: 310, Loss: 0.0232\n",
      "Epoch: 5, Step: 341, Loss: 0.0228\n",
      "Epoch: 5, Step: 372, Loss: 0.0228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m epochs, lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m0.001\u001B[39m\u001B[38;5;66;03m#epochs,lr = 20, 0.001\u001B[39;00m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 31\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, train_iter, test_iter, epochs, lr, device)\u001B[0m\n\u001B[0;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 31\u001B[0m     metric\u001B[38;5;241m.\u001B[39madd(l \u001B[38;5;241m*\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[43md2l\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m, X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     32\u001B[0m timer\u001B[38;5;241m.\u001B[39mstop()\n\u001B[0;32m     33\u001B[0m train_l \u001B[38;5;241m=\u001B[39m metric[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m metric[\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\Lib\\site-packages\\d2l\\torch.py:3195\u001B[0m, in \u001B[0;36maccuracy\u001B[1;34m(y_hat, y)\u001B[0m\n\u001B[0;32m   3193\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39margmax(y_hat, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   3194\u001B[0m cmp \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mastype(y_hat, y\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;241m==\u001B[39m y\n\u001B[1;32m-> 3195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(d2l\u001B[38;5;241m.\u001B[39mreduce_sum(d2l\u001B[38;5;241m.\u001B[39mastype(cmp, y\u001B[38;5;241m.\u001B[39mdtype)))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epochs, lr = 10, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "train(net, train_loader64, test_loader64, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ret50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T11:35:31.840593Z",
     "start_time": "2024-03-26T11:26:58.155624Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:[cuda]\n",
      "Epoch: 1, Step: 31, Loss: 4.4236\n",
      "Epoch: 1, Step: 62, Loss: 2.3907\n",
      "Epoch: 1, Step: 93, Loss: 1.6924\n",
      "Epoch: 1, Step: 124, Loss: 1.3206\n",
      "Epoch: 1, Step: 155, Loss: 1.0931\n",
      "Epoch: 1, Step: 186, Loss: 0.9404\n",
      "Epoch: 1, Step: 217, Loss: 0.8225\n",
      "Epoch: 1, Step: 248, Loss: 0.7367\n",
      "Epoch: 1, Step: 279, Loss: 0.6706\n",
      "Epoch: 1, Step: 310, Loss: 0.6135\n",
      "Epoch: 1, Step: 341, Loss: 0.5677\n",
      "Epoch: 1, Step: 372, Loss: 0.5307\n",
      "Epoch: 1, Step: 403, Loss: 0.4970\n",
      "Epoch: 1, Step: 434, Loss: 0.4678\n",
      "Epoch: 1, Step: 465, Loss: 0.4433\n",
      "Epoch: 1, Step: 496, Loss: 0.4200\n",
      "Epoch: 1, Step: 527, Loss: 0.3990\n",
      "Epoch: 1, Step: 558, Loss: 0.3808\n",
      "Epoch: 1, Step: 589, Loss: 0.3657\n",
      "Epoch: 1, Step: 620, Loss: 0.3508\n",
      "Epoch: 1, Step: 651, Loss: 0.3369\n",
      "Epoch: 1, Step: 682, Loss: 0.3246\n",
      "Epoch: 1, Step: 713, Loss: 0.3134\n",
      "Epoch: 1, Step: 744, Loss: 0.3034\n",
      "Epoch: 1, Step: 775, Loss: 0.2956\n",
      "Epoch: 1, Step: 806, Loss: 0.2877\n",
      "Epoch: 1, Step: 837, Loss: 0.2797\n",
      "Epoch: 1, Step: 868, Loss: 0.2721\n",
      "Epoch: 1, Step: 899, Loss: 0.2645\n",
      "Epoch: 1, Step: 930, Loss: 0.2578\n",
      "Epoch: 1, Step: 938, Loss: 0.2563\n",
      "Train Accuracy: 94.98%, Test Accuracy: 98.31%\n",
      "Epoch: 2, Step: 31, Loss: 0.0821\n",
      "Epoch: 2, Step: 62, Loss: 0.0722\n",
      "Epoch: 2, Step: 93, Loss: 0.0683\n",
      "Epoch: 2, Step: 124, Loss: 0.0621\n",
      "Epoch: 2, Step: 155, Loss: 0.0597\n",
      "Epoch: 2, Step: 186, Loss: 0.0587\n",
      "Epoch: 2, Step: 217, Loss: 0.0571\n",
      "Epoch: 2, Step: 248, Loss: 0.0590\n",
      "Epoch: 2, Step: 279, Loss: 0.0576\n",
      "Epoch: 2, Step: 310, Loss: 0.0562\n",
      "Epoch: 2, Step: 341, Loss: 0.0553\n",
      "Epoch: 2, Step: 372, Loss: 0.0539\n",
      "Epoch: 2, Step: 403, Loss: 0.0556\n",
      "Epoch: 2, Step: 434, Loss: 0.0577\n",
      "Epoch: 2, Step: 465, Loss: 0.0578\n",
      "Epoch: 2, Step: 496, Loss: 0.0565\n",
      "Epoch: 2, Step: 527, Loss: 0.0563\n",
      "Epoch: 2, Step: 558, Loss: 0.0552\n",
      "Epoch: 2, Step: 589, Loss: 0.0545\n",
      "Epoch: 2, Step: 620, Loss: 0.0542\n",
      "Epoch: 2, Step: 651, Loss: 0.0548\n",
      "Epoch: 2, Step: 682, Loss: 0.0554\n",
      "Epoch: 2, Step: 713, Loss: 0.0567\n",
      "Epoch: 2, Step: 744, Loss: 0.0568\n",
      "Epoch: 2, Step: 775, Loss: 0.0573\n",
      "Epoch: 2, Step: 806, Loss: 0.0576\n",
      "Epoch: 2, Step: 837, Loss: 0.0579\n",
      "Epoch: 2, Step: 868, Loss: 0.0581\n",
      "Epoch: 2, Step: 899, Loss: 0.0586\n",
      "Epoch: 2, Step: 930, Loss: 0.0589\n",
      "Epoch: 2, Step: 938, Loss: 0.0589\n",
      "Train Accuracy: 98.22%, Test Accuracy: 98.43%\n",
      "Epoch: 3, Step: 31, Loss: 0.0417\n",
      "Epoch: 3, Step: 62, Loss: 0.0391\n",
      "Epoch: 3, Step: 93, Loss: 0.0400\n",
      "Epoch: 3, Step: 124, Loss: 0.0438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m epochs, lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m0.001\u001B[39m\u001B[38;5;66;03m#epochs,lr = 20, 0.001\u001B[39;00m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret50\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 32\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, train_iter, test_iter, epochs, lr, device)\u001B[0m\n\u001B[0;32m     30\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 32\u001B[0m     metric\u001B[38;5;241m.\u001B[39madd(l \u001B[38;5;241m*\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[43md2l\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m, X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     33\u001B[0m timer\u001B[38;5;241m.\u001B[39mstop()\n\u001B[0;32m     34\u001B[0m train_l \u001B[38;5;241m=\u001B[39m metric[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m/\u001B[39m metric[\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\Lib\\site-packages\\d2l\\torch.py:3195\u001B[0m, in \u001B[0;36maccuracy\u001B[1;34m(y_hat, y)\u001B[0m\n\u001B[0;32m   3193\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39margmax(y_hat, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   3194\u001B[0m cmp \u001B[38;5;241m=\u001B[39m d2l\u001B[38;5;241m.\u001B[39mastype(y_hat, y\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;241m==\u001B[39m y\n\u001B[1;32m-> 3195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(d2l\u001B[38;5;241m.\u001B[39mreduce_sum(d2l\u001B[38;5;241m.\u001B[39mastype(cmp, y\u001B[38;5;241m.\u001B[39mdtype)))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epochs, lr = 10, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train(ret50, train_loader, test_loader, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ret50_plot_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T13:31:21.283576Z",
     "start_time": "2024-04-10T13:31:10.524402Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 22.00 GiB of which 0 bytes is free. Of the allocated memory 25.34 GiB is allocated by PyTorch, and 15.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret50\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 23\u001B[0m, in \u001B[0;36mtrain_loss\u001B[1;34m(net, train_iter, test_iter, epochs, loss, device, lr)\u001B[0m\n\u001B[0;32m     21\u001B[0m trainer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     22\u001B[0m X, y \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 23\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m l \u001B[38;5;241m=\u001B[39m loss(y_hat, y)\n\u001B[0;32m     25\u001B[0m l\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001B[0m, in \u001B[0;36mReLU.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1471\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1472\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1473\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 22.00 GiB of which 0 bytes is free. Of the allocated memory 25.34 GiB is allocated by PyTorch, and 15.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "epochs, lr = 20, 0.001#epochs,lr = 20, 0.001\n",
    "device = torch.device(\"cuda:0\")\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# train_loss(net, train_iter, test_iter, num_epochs, loss, trainer, device, lr\n",
    "train_loss(ret50, test_loader64, test_loader64, epochs, loss, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    " \n",
    "\n",
    " \n",
    "# ... 模型训练代码 ...\n",
    " \n",
    "# 清理模型和优化器占用的内存\n",
    "del ret50\n",
    "del trainer\n",
    " \n",
    "# 清理PyTorch缓存\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T13:31:59.527625Z",
     "start_time": "2024-04-10T13:31:59.515632Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 可视化展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = net\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "costs = []\n",
    "\n",
    "# 训练网络\n",
    "def train(epoch):\n",
    "    batch_loss = 0\n",
    "    for step, (data) in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_func(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        costs.append(loss)\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "        if step % 300 == 299:\n",
    "            print(f'epoch:{epoch},step:{step + 1},mini_loss:{batch_loss / 300:.3f}')\n",
    "            batch_loss = 0\n",
    "\n",
    "\n",
    "# 测试网络\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader64:\n",
    "            inputs, labels = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            output = model(inputs)\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "    print(f'测试集上的准确率为:{correct / total * 100:.3f}%')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(3):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    # 绘制图片:此时的costs里面的数据是tensor类型,如果是gpu上跑的就是cuda,需要将他转换成array\n",
    "    print(costs)\n",
    "    if torch.cuda.is_available():\n",
    "        costs = [cost.cpu().detach().numpy() for cost in costs]\n",
    "    else:\n",
    "        costs = [cost.numpy() for cost in costs]\n",
    "    print(costs)\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('number of iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Inception Net')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
